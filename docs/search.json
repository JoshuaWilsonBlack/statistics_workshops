[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NZILBB R and Open Science Workshops",
    "section": "",
    "text": "Preface\nThis ‘book’ will slowly evolve and add contributors as the NZILBB R and Open Science workshops develop. The hope is that this resource will allow people to study at their own pace.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "From the first semester of 2024, we will be running a series of workshops on R at NZILBB. This document will contain materials for the introductory sessions, with another collection for the more advanced sessions.\nThe focus of the workshops is on R and Open Science. The aim is to get you up to speed with the use of R and associated Open Science infrastructure so that your research is maximally reproducible, replicable, and helpful to the wider scientific community. This requires us to learn some programming!\nThis resource will gradually grow as I write up workshops. The ideal is that it will allow people to follow the material at their own pace and/or to catch up to wherever we are at in our tutorial sessions.\nFor UC students and staff, I am happy to talk over any issues you have with this material. Please get in touch with me at joshua.black@canterbury.ac.nz.\nIf you have found your way to this material by some other means, you can also email me!\n\nOther resources\nAt this stage, this is a somewhat disorganised list of texts and video which you may find interesting.\n\nThe alternative to data analysis with a programming language is usually some kind of spreadsheet. Here are some spreadsheet horror stories: https://eusprig.org/research-info/horror-stories/.\nWhy do we have to learn how to program? Why is science ‘amateur software development’? This is a good lecture on the topic: https://www.youtube.com/watch?v=8qzVV7eEiaI.\n\nUsually these techniques aren’t explicitly taught. These workshops are our attempt to respond to this problem!\n\nWhy can’t you do data science with a spreadsheet? https://www.youtube.com/watch?v=cpbtcsGE0OA\nThese workshops have been heavily influenced by Winter (2019)\n\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "1.1 Install R and RStudio\nR is a programming language. RStudio is a piece of software for interacting with R.\nYou don’t have to use RStudio in order to use R, but we will assume you are using it in these workshops.1\nTo install R and RStudio on your own device follow the steps at https://posit.co/download/rstudio-desktop/.\nTo install R and RStudio on a University of Canterbury device:\nInstalling RStudio on a University of Canterbury device will also install R.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#install-r-and-rstudio",
    "href": "getting_started.html#install-r-and-rstudio",
    "title": "1  Getting Started",
    "section": "",
    "text": "Windows: open the “Software Center”, search for RStudio, and press the install button.\nMac: open “UC Self Service”, search for RStudio, and press the install button.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#open-rstudio",
    "href": "getting_started.html#open-rstudio",
    "title": "1  Getting Started",
    "section": "1.2 Open RStudio",
    "text": "1.2 Open RStudio\nIf you have installed RStudio, if should now appear in your start menu on Windows, and your Applications folder and launchpad if you are on macOS. Open it. You should see something like this:\n\n\nThe RStudio interface has four primary ‘panes’. Only three of these will be visible when you first open RStudio. The largest pane is the console pane. It is usually on the bottom left of the RStudio window, but currently takes up the entire left side. We also see the environment pane at the top right and the output pane at the bottom right.2\nThe console pane should have a message telling you what version of R you are using and the platform you are on (i.e. your hardware and operating system). This is what you would see if you opened R by itself.3\nThe environment pane should be empty. You will see multiple tabs across the top of this pane. The environment tab will allow us to see the data which we are working with at a given time. At this stage, you may see a tab labelled ‘Tutorial’. I’ll tell you how to use this later (Section 1.6).\nThe output pane will start by showing you a list of files on your computer. This is useful for finding and manipulating files (just like a file browser) In future, it is also where plots and help pages will appear.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#interact-with-r-in-the-console",
    "href": "getting_started.html#interact-with-r-in-the-console",
    "title": "1  Getting Started",
    "section": "1.3 Interact with R in the Console",
    "text": "1.3 Interact with R in the Console\nWe will get started by interacting with R in the console pane. You should see a &gt; in the console pane. We can enter code here. If the code works, we will see the output immediately below (or perhaps in the output pane, depending on the code). If the code doesn’t work, an error message will appear.\n\n1.3.1 Basic arithmetic\nWe’ll start with some basic arithmetic. We add two numbers together by writing the first number, the + sign, and the second number. Enter the code in the box below after the &gt; in your console. The expected output appears below the box. You should see the same thing in your console after you press enter/return.\n\n1 + 1\n\n[1] 2\n\n\nThe other basic arithmetic operators work in the same way. Subtraction:\n\n500 - 49\n\n[1] 451\n\n\nWe use * for multiplication. We enter real numbers by using a decimal point.\n\n43 * 6.4\n\n[1] 275.2\n\n\nFor exponentiation we use ^ (usually, shift + 6).\n\n924^5\n\n[1] 6.735345e+14\n\n\nThe output given here is in scientific notation. It is important to be able to read this notation when using R. It makes very very small and very very large numbers much easier to write and is often used in the output of R functions. To convert from scientific notation to regular digits, multiple the number which appears before the e by 10 to the power of the number after the e. In this case, we take the number \\(6.735345\\) and multiply it by \\(10^{14}\\) to get \\(673,534,500,000,000\\). That is, six hundred seventy-three trillion and a bit. According to Wikipedia, this is something like the total number of cells in six and a half adult humans and a bit fewer than the number of ants on Earth.\nThere are a few different operators associated with division. Usually, you will want to use /. e.g.:\n\n43 / 7\n\n[1] 6.142857\n\n\nSometimes, it is useful to get the integer component on the answer or the remainder. If we want the integer, we use %/%:\n\n43 %/% 7\n\n[1] 6\n\n\nIf we want the remainder, we use:\n\n43 %% 7\n\n[1] 1\n\n\nThat is, if we divide 43 by 7, we get 6 groups of 7, with 1 remaining.\nComputer programming requires attention to minor details of punctuation and spacing. Hours can be spent trying to discover why code is not working, only to discover a missing comma. This is especially true in the early stages of learning, where error messages can be very confusing.\nIt is worth knowing when you can add spaces and when you can’t. The spaces in the code above between the numbers and the arithmetic operators are not necessary. So, for instance, you could write:\n\n43/7\n\n[1] 6.142857\n\n\nIn fact, you can add however many spaces you like!\n\n34  /    2\n\n[1] 17\n\n\nThe only reason to prefer one over the other is readability. This raises the issue of code style, which we will discuss in future workshops. Note that, above, there wasn’t a space in 924^5—this is a style convention for ^ and some other (‘high precedence’) operators which we will encounter later.4\n\n\n1.3.2 Vectors and Variables\nWe work with large collections of experimental data or values derived from corpora. But the commands we’ve looked at above only deal with two numbers at a time. The simplest structure for dealing with more than one value is a vector.\nWe create vectors using the function c(). The c() function combines values in to a vector.\n\nc(1, 2, 3, 4)\n\n[1] 1 2 3 4\n\n\nThe [1] you see in the output is followed by the first element of the vector. If you print out a very long vector you will see numbers other than 1 inside the square brackets. For instance:\n\n60:124\n\n [1]  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n[20]  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n[39]  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n[58] 117 118 119 120 121 122 123 124\n\n\nThe : operator produces a vector from the first number to the second number (inclusive) in steps of one. The resulting output is long enough that it has to go across two lines. When the new line starts you will see another number in square brackets. This tells you how far through the vector you are at the line break. Exactly which number it is will vary according to the system you are using. For me, currently editing this text in RStudio, it is [38]. That is, the number which follows [38] is the 38th number in the vector.\nWe call the values in a vector the elements of the vector. The elements of a vector have to be the same type of thing. We’ll talk about types more later. For now, just note that a number is a different kind of thing from a string of characters. So, what happens if we try to mix numbers and strings in a vector?\n\nc(1, 2, 3, \"dog\")\n\n[1] \"1\"   \"2\"   \"3\"   \"dog\"\n\n\nR hasn’t explicitly complained, but it has done something without telling you what it has done. The numbers we entered now have quotation marks around them. They have been turned in to strings. Keep an eye out for quotation marks — sometimes you might think you are dealing with numbers, but really you are dealing with strings. This is a common problem when loading your own data.\nWhy worry? Well, your code likely won’t work if you have strings rather than numbers. For instance, you can’t apply arithmetic operators to strings.\n\n\"1\" + \"2\"\n\nError in \"1\" + \"2\": non-numeric argument to binary operator\n\n\nThe above is the first error message you’ve seen in this course. You will see many more in your time working with R. The error message is telling you that what you are doing does not work on anything but numbers.\nTo enter a string, you can use either double quotes or single quotes.\nVectors can also be used for arithmetic. Under the hood, statistics is mostly arithmetic with collections of vectors. How are these arithmetic operations implemented in R?\nThe simplest case is when we use a vector and a single number, as follows\n\n2 * c(1, 2, 3, 4)\n\n[1] 2 4 6 8\n\n\nEach element of the vector has been multiplied by \\(2\\). The same is true of addition, division, and subtraction. These are ‘element-wise’ operations. That is, they are applied to each element individually.\n\n3 / c(1, 2, 3, 4)\n\n[1] 3.00 1.50 1.00 0.75\n\n\nThis also works with two vectors. For instance:\n\nc(1, 2, 3, 4) * c(1, 2, 3, 4)\n\n[1]  1  4  9 16\n\n\nHere we get the first elements multiplied together, then the second, then the third, and so on.\nIf one vector is shorter than the other, is will be ‘recycled’ to match the longer vector:\n\nc(1, 2) * c(1, 2, 3, 4)\n\n[1] 1 4 3 8\n\n\nYou do not want to be entering the same vector over and over again. This is where variables come in. Variables allow us to associate names with values.\nTo assign an object to a name, we use &lt;-. For instance:\n\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\n\nNow the name my_cool_vector is associated with the vector c(6, 9, 4, 5, 2, 2). If you look to the top right of the RStudio window you should now see this variable in your environment pane. The name will be on the left and the value on the right.\n\n\n\nOur cool vector in the RStudio environment pane. Your screen may look a little different.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn most cases you can also use = to assign an object to a name. This may seem more natural to you if you are coming from another programming languages. The convention is to use &lt;-. Sometimes = takes on a different function, but &lt;- is always assignment of an object to a name.\n\n\nWe can now apply operations using the name. For instance:\n\n4 * my_cool_vector\n\n[1] 24 36 16 20  8  8\n\n\nTo see what object is associated with a name we can look in the environment pane or simply enter the name into the console.\n\nmy_cool_vector\n\n[1] 6 9 4 5 2 2\n\n\nWe can also look up specific elements using square brackets. If we wanted to look up the fourth element in my_cool_vector we would enter the following code.\n\nmy_cool_vector[4]\n\n[1] 5\n\n\nWe can even change elements by using the assignment opperator (&lt;-).\n\nmy_cool_vector[4] &lt;- 3\nmy_cool_vector\n\n[1] 6 9 4 3 2 2\n\n\nThe fourth entry in the vector is now 3 rather than 5.\nNaming variables is serious business. It is important to know what you could do and what you should do.\nR institutes the following rules for names:\n\nA name must consist of letters, digits, ., and _.\nIt cannot begin with digits or _.\nIt cannot come from a list of reserved words (e.g. TRUE — these names have important roles in R and can’t be overridden.)\n\n\n\n\n\n\n\nNote\n\n\n\nWhat counts as a ‘letter’ varies by operating system and local settings (your ‘locale’). The recommendation from Hadley Wickham is that you only use ASCII letters (for instance, avoid use of any diacritics/accents).5\nOne local reason you might want to use non-ASCII characters is if you want to use te reo Māori with macrons for your variable names. This might be appropriate for a particular project (the question is always who you want to share your code with). Pretty much anyone using a modern operating system should be able to use your code. You may decide that the small risk of incompatibility is worth it in this case.\n\n\nI follow the convention of using _ rather than . in my variable names. So, e.g., I’d prefer my_cool_vector over my.cool.vector. This reduces ambiguity in some cases.\nWhile we are talking about naming, R will accept anything placed within backticks (‘`’) as a variable name. If you have a chaotic temperament, you might decide to use variable names like this:\n\n# Eldritch variable\n`t̸̡͚̳͓̜̘̪̙̟̣͛̋̈̐͜ḩ̷̛̗̬̪̔̾͋̌̂̓͑̔̚͝ë̵̮̟̟̼̲̦͙̠̟́͋̇̏̓ ̶̟̱̲̠͎̙̠̆̑̈́̉̆̏̋͠͠t̷̲͉͔̘̬̪͖̗́͌̏̉̏̄͊̍̽͋̈̈́̀͝͠o̵͚͙̮͙͉̱̱͕̗̘̻͋͋͋̀́̒͝ͅw̸͖͚̖̣̭̥͍̹͚̞͕̺͇͙͌͛͋̆̿̈́̎̆̋̑͌̏͘͠͝e̵͖̝̞̙͕̤̅̃̓r̴͍̼̱̜̹͚̎̌̂͆͗̏́ṡ̷͔͉͇͗̍̆̔̕ͅ ̷̪̱̞͈̰̈́͜ǫ̷̤͍̫̠̻̣̪̻͖̒̈́͐͂̿̆̑̄̂͘f̶̠͉̯̪̪̖̦͋͝ ̶̙̻̝͆̈͠C̴̳̪̪̻̫̬̳̜̅͑̇͌̆̕a̶̡͚̼͍̺͂̈́̄r̷̨̛̛̜̹͙̲̝̲̖͍̓̊͒̄̓̏͂͐͛͑̊͘c̸͇̲̲͈͕͉͍̗̐ơ̵̟̠̒̔͑͆s̶̨̢̱̱̲͇͉̪̻̖̠͊̈́̐͋́̈́͜a̸̗̩̯̳̝͈̰̅͒̂̏͛̽̓͑̈́̾ͅ ̷̢͎͎̳̖̤̥̜̀̑̈́̈́r̴̦͌͛͘o̴̩̩̯̤̝̊͗̿̉͗͂͂̆̈́͘s̶͔̼̞̱̻̭̻͑̔͛̔ḙ̸̢̀̎͗̓͊̈̊̉̚̚͝ ̸̠̰̞̬̐̆̽̅̀̈̂̌͠b̶̧̜̟͍͔̘̥͇̈́͒̃͒̈́͊̓̉́̉̐͘͘͝͝ę̵͚̀̈́̿̌̆̈́͘̕͠͝ͅh̸̛͎̱͚͕̹̘̥̠͕̟̼͝ͅî̶̞̹̺̰̎̿̊̽͒͑͑̽͝n̵̢̢̛̛̟͓̗̮̦̪̥̩͓̪̘͗͗̑̊̌̉̂͊͠͝d̵͎̭̤̲͋͌̃̎̊ ̷̧̧̛̤͇̫̝̗̻͚̐̊̈́̇̂͗̋t̵͓̻̦̻̗͇̜̼̻̫̼̭̄́͘̚h̵̨̅̉̄e̸̡̡̨̞̪̝̝̟͔̞̞͔̰̒̓͆̐͛̂̒͂̊̆̽̃̌͘ ̴̛̦̖̖̖̹̖̹̣̳̕m̶̡͉̦̣͉̳̪͖͕͍͙̪̟͌̍̏͆̐̄̂̚͘o̸̭̯̠̭͎̖͐͗̏̉͋̅͊̓̓̂̏̓̏̍͝ǫ̴͖͈̖̣̤͍̝̩̳̪̔͂̋̄̑̏̒̏̏̈́ñ̸̙̪͉͓̼̯̩͋̋̌̏̃͘̕͘.̵̙̮̾̐͠ͅ` &lt;- 10\n\n# Spooky variable\n`👻` &lt;- 5\n\nAnd you could even do some maths with these variables:\n\n`t̸̡͚̳͓̜̘̪̙̟̣͛̋̈̐͜ḩ̷̛̗̬̪̔̾͋̌̂̓͑̔̚͝ë̵̮̟̟̼̲̦͙̠̟́͋̇̏̓ ̶̟̱̲̠͎̙̠̆̑̈́̉̆̏̋͠͠t̷̲͉͔̘̬̪͖̗́͌̏̉̏̄͊̍̽͋̈̈́̀͝͠o̵͚͙̮͙͉̱̱͕̗̘̻͋͋͋̀́̒͝ͅw̸͖͚̖̣̭̥͍̹͚̞͕̺͇͙͌͛͋̆̿̈́̎̆̋̑͌̏͘͠͝e̵͖̝̞̙͕̤̅̃̓r̴͍̼̱̜̹͚̎̌̂͆͗̏́ṡ̷͔͉͇͗̍̆̔̕ͅ ̷̪̱̞͈̰̈́͜ǫ̷̤͍̫̠̻̣̪̻͖̒̈́͐͂̿̆̑̄̂͘f̶̠͉̯̪̪̖̦͋͝ ̶̙̻̝͆̈͠C̴̳̪̪̻̫̬̳̜̅͑̇͌̆̕a̶̡͚̼͍̺͂̈́̄r̷̨̛̛̜̹͙̲̝̲̖͍̓̊͒̄̓̏͂͐͛͑̊͘c̸͇̲̲͈͕͉͍̗̐ơ̵̟̠̒̔͑͆s̶̨̢̱̱̲͇͉̪̻̖̠͊̈́̐͋́̈́͜a̸̗̩̯̳̝͈̰̅͒̂̏͛̽̓͑̈́̾ͅ ̷̢͎͎̳̖̤̥̜̀̑̈́̈́r̴̦͌͛͘o̴̩̩̯̤̝̊͗̿̉͗͂͂̆̈́͘s̶͔̼̞̱̻̭̻͑̔͛̔ḙ̸̢̀̎͗̓͊̈̊̉̚̚͝ ̸̠̰̞̬̐̆̽̅̀̈̂̌͠b̶̧̜̟͍͔̘̥͇̈́͒̃͒̈́͊̓̉́̉̐͘͘͝͝ę̵͚̀̈́̿̌̆̈́͘̕͠͝ͅh̸̛͎̱͚͕̹̘̥̠͕̟̼͝ͅî̶̞̹̺̰̎̿̊̽͒͑͑̽͝n̵̢̢̛̛̟͓̗̮̦̪̥̩͓̪̘͗͗̑̊̌̉̂͊͠͝d̵͎̭̤̲͋͌̃̎̊ ̷̧̧̛̤͇̫̝̗̻͚̐̊̈́̇̂͗̋t̵͓̻̦̻̗͇̜̼̻̫̼̭̄́͘̚h̵̨̅̉̄e̸̡̡̨̞̪̝̝̟͔̞̞͔̰̒̓͆̐͛̂̒͂̊̆̽̃̌͘ ̴̛̦̖̖̖̹̖̹̣̳̕m̶̡͉̦̣͉̳̪͖͕͍͙̪̟͌̍̏͆̐̄̂̚͘o̸̭̯̠̭͎̖͐͗̏̉͋̅͊̓̓̂̏̓̏̍͝ǫ̴͖͈̖̣̤͍̝̩̳̪̔͂̋̄̑̏̒̏̏̈́ñ̸̙̪͉͓̼̯̩͋̋̌̏̃͘̕͘.̵̙̮̾̐͠ͅ` + `👻`\n\n[1] 15\n\n\nUnsurprisingly, if you try this without the backticks, you will get an error:\n\n👻 &lt;- 5\n\nError: &lt;text&gt;:1:1: unexpected invalid token\n1: 👻\n    ^\n\n\nDo not take advantage of backticks to use names like this.\nWhy am I even telling you about backticks? They often appear in practice as a result of importing data from a spreadsheet. Usually they appear because the column names in the spreadsheet have spaces in them. One of the first things to do when tidying up data loading from a spreadsheet is to change the names.\n\n\n1.3.3 Exercises\n\nWhat is the output of 5:10?\n\n c(5, 10) [1]  5  6  7  8  9 10 [1]  5  6  7  8  9\n\nWhat is the output of 10 * c(1, 2)?\n\n [1] 10 20 [1]  30 Error in 10 * c(1, 2) : non-numeric argument to binary operator\n\nWhat is the output of c(3, 4, 6, 2)[2]?\n\n [1] 4 [1] 3 4 6 2\n\nLook at the variable names in the following list. Some of them are very bad names for stylistic reasons, but will they be accepted by R? I.e., are they syntactically valid?\n\nnz_vowels TRUEFALSE\n_nz_vowels TRUEFALSE\n🥝_vowels TRUEFALSE\n`🥝_vowels` TRUEFALSE\nTraditional languages should be taught in school TRUEFALSE\nTraditional.languages_should_be.taught.in_school TRUEFALSE\nin_school TRUEFALSE\n5_points_attitude TRUEFALSE\nattitude_5 TRUEFALSE\n::::: TRUEFALSE\nfunction TRUEFALSE",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#start-an-r-script",
    "href": "getting_started.html#start-an-r-script",
    "title": "1  Getting Started",
    "section": "1.4 Start an R Script",
    "text": "1.4 Start an R Script\nIf we exclusively used R in the console, we would be in no better position than if we just used Excel or another spreadsheet programme. We want to be able to retrace our steps.\nIn order to start an R script go to File &gt; New File &gt; R Script or use the keyboard shortcut Cmd + Shift + N (macOS) or Ctrl + Shift + N (Windows).\nThis will open the source pane. We will now enter code in the source pane rather than the console.6\nYou can enter R code into a script in the same way you have been adding it to the console. Unlike the console, each command is saved in the script and pressing return/enter will not run the code.\nUsually you will run code by selecting it and pressing Cmd + Return (macOS) or Ctrl + Enter (Windows).7 If you have no code selected, this command will run the line which your cursor is on. The alternative is to run the entire script all at once. This can be done by pressing Source at the top right of the source pane.8\nCopy and paste the following into your new script then run it. You should see the output in the console pane.\n\n\n\n\n\n\nNote\n\n\n\nThere is some wisdom to the idea that you should get coding ‘into your fingers’ by typing out examples yourself. This may be true, but it is up to you! I have enabled the ‘copy/paste’ button in all code blocks. You should see a clipboard icon when you have your cursor over a code block. Click the clipboard to copy the code.\n\n\n\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\nmy_cool_vector ^ 2\n\n[1] 36 81 16 25  4  4\n\n\n\n\n\nThe source pane with our script entered. Note the ‘Source’ button at the right.\n\n\nIt is important to leave comments, so that your code can be interpreted by other researchers (including yourself in the future!). Anything which appears after a # is a comment and will be ignored by R.\nWe could change our script as follows, and the result will be identical:\n\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\n# Square each element of my cool vector and output to console.\nmy_cool_vector ^ 2\n\n[1] 36 81 16 25  4  4\n\n\nIn actual data analysis projects, commenting is vital. We’ll see some more useful examples of commenting as we go on.\n\n1.4.1 Matrices and Dataframes\nIn data analysis we want to find associations between multiple variables. So single vectors aren’t going to cut it. We need collections of vectors.\nThe simplest version of this is a matrix. Matricies are like vectors in that they can only contain elements of the same type.\nAdd the following lines to your script and run them:\n\nmy_cool_matrix &lt;- matrix(my_cool_vector, nrow = 3)\nmy_cool_matrix\n\n     [,1] [,2]\n[1,]    6    5\n[2,]    9    2\n[3,]    4    2\n\n\nWe now have a \\(3\\times2\\) matrix of numbers.\nSquare brackets are again used to manipulate individual elements. But we now have to include both rows and columns. If we want the second column and third row we can use my_cool_matrix[3, 2]. If you want the entire second column you would enter my_cool_matrix[, 2]. If you want the entire first row, you would enter my_cool_matrix[1, ]. Try out these commands in either the script or the console window.\nHave a look in the environment pane. You should now see a separation between “data” and “values”. The “data” section contains structured objects, such as matrices. The one practical difference here is that if you click on something in the data section it will usually open in a new tab. In this case, you’ll see something that looks a lot like a spreadsheet application.\nOften our data will include elements of multiple different types. For instance, it might include numbers indicating the age of a participant, or which of a series of options they chose. It might also include words (strings) indicating which experimental condition they are in, or a transcript of an interview. Matricies can’t handle this, but data frames can.\nWe create data frames using the data.frame function. Enter the following into your script and run it:\n\n# Re-enter the vector to revert modifications made above.\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\n\nmy_data_frame &lt;- data.frame(\n  \"numbers\" = my_cool_vector,\n  \"letters\" = c(\"N\", \"Z\", \"I\", \"L\", \"B\", \"B\")\n)\n\nmy_data_frame\n\n# A tibble: 6 × 2\n  numbers letters\n    &lt;dbl&gt; &lt;chr&gt;  \n1       6 N      \n2       9 Z      \n3       4 I      \n4       5 L      \n5       2 B      \n6       2 B      \n\n\nWe now have a data frame with a column of numbers and a column of corresponding letters. We have also given each of these columns a name ( ‘numbers’ for the column of numbers and ‘letters’ for the column of letters). Each row of the data frame is an observation, and each column is a variable.9 Perhaps you can figure out what the association between the two variables is.\nWhen we have names for columns, we can access the column using the name by means of the $ symbol:\n\nmy_data_frame$numbers\n\n[1] 6 9 4 5 2 2\n\n\nNote the use of a comment to explain why we are re-creating my_cool_vector. This is the kind of step in the middle of a script which is likely to cause confusion without a comment.\n\n\n1.4.2 Functions and Help\nWe have now seen a few functions (matrix to create a matrix, and data.frame to create a data frame). Functions are what we use to perform data analysis tasks in R. To apply a function, we writes its name and then enter a series of arguments inside brackets. The arguments are the information which we pass for the function in order for it to do its work.\nRecall the matrix code from above:\n\nmy_cool_matrix &lt;- matrix(my_cool_vector, nrow = 3)\n\nHere the matrix function is given two arguments. The first is the vector my_cool_vector and the second is nrow = 3. The two arguments are separated by a comma. If we want to work out where these came from, we need to look at the help page for the function. To do that enter the following in either the script or the console:\n\n?matrix\n\nYou should now see the help screen in the output pane (bottom right). This help page tells you about three related functions. In the ‘usage’ section, you will see some code examples which use the functions. The section arguments tells you what you can include as an argument to the function. So, for instance, you see that the nrow argument expects you to tell it how many rows you want the matrix to have.\nThe functions come in the order they appear in the usage example. In this case, each of the possible arguments is named. So, for instance, the first argument is called data. We did not enter this explicitly when we used the argument. But we could have:\n\nmy_cool_matrix &lt;- matrix(data = my_cool_vector, nrow = 3)\n\nSometimes it makes your code more clear to include a name, like this.\nThe help screen also shows the default values for these argument. If there are default values, then you don’t need to manually enter every argument. If you are modifying only one argument, which appears latter in the list, then you will need to use the name.\nFor instance, if we wanted to say the matrix has two columns (rather than three rows), we would have to add ncol:\n\nmy_cool_matrix &lt;- matrix(my_cool_vector, ncol = 3)\nmy_cool_matrix\n\n     [,1] [,2] [,3]\n[1,]    6    4    2\n[2,]    9    5    2\n\n\nIf we didn’t, R would not know what argument we intended to modify. If you look in the ‘details’ section of the help page, you will see that if we only specify a number of rows or a number of columns, it will attempt to work out the other value.\n\n\n1.4.3 Install and Use a Package\nOne of the great advantages of R is that it has a large community of developers making packages to share their code. Packages allow us to cumulatively build on each others work and to do things quickly which might otherwise take a lot of time and statistical knowledge to achieve.\nWell start with a silly package: cowsay.10 This package produces text art animals who will ‘say’ whatever text you enter.\nTo install a package, enter the following to the console:\n\ninstall.packages('cowsay')\n\nThis means that the package cowsay is now installed on your computer. To use it in a script, you need to enter the following at the top of your script:\n\nlibrary(cowsay)\n\nBy convention, we add libraries at the start of a script. This lets other researchers see exactly what is needed to run the script at the start. In addition, packages sometimes conflict with one another, and it is important to see this before we carry out any data analysis.\nTo see what functions cowsay has, look at the documentation. If you want to see the names of functions, you can enter cowsay:: and RStudio will suggest the names of functions. You can add a ? in front of any of these function names to see the help file for the function.\nThere are two functions which come with cowsay: say and endless_horse. The function we will use is called say, so enter ?say in the console.\nNow enter the following into your script:\n\nsay(\n  what = \"\", # Write your own quote here (between the quotation marks)\n  by = \"\" # Enter a 'type of thing' from the list on the help page.\n)\n\nHere, I have used comments to indicate what you need to do to complete the code.\nHere’s one possible answer:\n\nsay(\n  what = \"It sure is lonely down here.\",\n  by = \"whale\"\n)\n\n\n            ------ \n           It sure is lonely down here. \n            ------ \n               \\   \n                \\  \n                 \\\n     .-'          \n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`\n\n\nSometimes packages contain data. This is one day to get data in to your analysis.\nIn fact, there’s plenty of data built in to R. Often this is used to demonstrate different functions. To see these, enter data() in the console. You can load one of these datasets in to your script by entering the name of the dataset as an argument to the function data(). The following code block loads up one of these datasets.\n\ndata(warpbreaks)\n\n# To view the data in RStudio, use `View` instead of `head`.\nhead(warpbreaks)\n\n# A tibble: 6 × 3\n  breaks wool  tension\n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  \n1     26 A     L      \n2     30 A     L      \n3     54 A     L      \n4     25 A     L      \n5     70 A     L      \n6     52 A     L      \n\n\nThere datasets also have help pages (see ?warpbreaks). What does this data represent?\nAs a final bit of R code, and to show you another function in action, let’s plot the numeric information in warpbreaks:\n\nhist(warpbreaks$breaks)\n\n\n\n\n\n\n\n\nHere we see the distribution of the count of warp breaks while weaving for a fixed length of yarn.\n\n\n1.4.4 Exercises\n\nWhat is the output of # 2 + 2?\n\n [1] 4 2 + 2 Nothing.\n\nConsider the following code to create a matrix:\n\nmatrix(\n  data = 1:50,\n  ncol = 5\n)\n\nHow many columns does the matrix have? 105Not enough information.\nHave a look at the documentation for the function say(), are the following statements true or false:\n\nThe default argument value for by is \"cat\". TRUEFALSE\nKākapo is an option for the by argument. TRUEFALSE\nIf you enter what = \"catfact\" the animal will say \"catfact\". TRUEFALSE.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#modify-rstudio-defaults",
    "href": "getting_started.html#modify-rstudio-defaults",
    "title": "1  Getting Started",
    "section": "1.5 Modify RStudio Defaults",
    "text": "1.5 Modify RStudio Defaults\nThere are many useful options which you might want to change to improve your RStudio experience. These can be found at Tools &gt; Global Options.\nI’m going to assert that you should change some settings in the ‘General’ window which will have appeared for you without properly explaining myself.11 Make it so that your settings match the following image:\n\n\n\nThe desired state of the General settings.\n\n\nThis means that nothing will be saved between times when you open R. This, in turn, means that your script has to contain everything that is important for your analysis and you will not accidentally rely on something being carried over between programming sessions. What do I mean? Well, you might run a piece of code and then accidentally delete it from the script. The results of running the code could hang around between sessions and you would not notice your mistake. When it comes to sharing your code, the research you share it with will not be able to run it successfully and it might take a long time to discover the problem.\n\n\n\n\n\n\nWarning\n\n\n\nIf you have been using R for a while, you may be relying on R to keep the result of long computations between sessions. If so, leave the settings as they are for now and talk to me (Josh). There are a few ways to save computations so that you do not have to, say, refit a large model from scratch every time.\n\n\nYou should also modify the appearance of RStudio to your liking using the Appearance options.\n\n\n\nThe appearance pane in General Options",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#sec-gs-additional",
    "href": "getting_started.html#sec-gs-additional",
    "title": "1  Getting Started",
    "section": "1.6 Additional Resources",
    "text": "1.6 Additional Resources\n\nThere are many good R and RStudio tutorials out there. One, which you can use from within RStudio is learnr. Install the package by entering install.packages('learnr') and you should see the tutorials in the environment pane.\nThe material which you get in learnr is from the book R for Data Science, available here: https://r4ds.hadley.nz/\nSee the first chapter of Winter (2019)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#sec-alternatives",
    "href": "getting_started.html#sec-alternatives",
    "title": "1  Getting Started",
    "section": "1.7 Alternatives to RStudio",
    "text": "1.7 Alternatives to RStudio\nYou can write R code in any text editor which you like. Popular options with more or less integration of R include:\n\nVisual Studio Code\nESS (i.e. Emacs Speaks Statistics).\n\nWe won’t discuss these alternatives in these workshops. The most likely reason for you to use one of them is that you are already a keen programmer with strong preferences concerning your tools.\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#footnotes",
    "href": "getting_started.html#footnotes",
    "title": "1  Getting Started",
    "section": "",
    "text": "For alternatives which you might explore see Section 1.7↩︎\nYou will find different terminology out there. I’m following the language in the official RStudio User Guide: https://docs.posit.co/ide/user/ide/guide/ui/ui-panes.html.↩︎\nTry this. You should find a shortcut to open R in the Start menu on Windows or the Launchpad in macOS. On Linux or macOS you can also open a terminal window, type ‘r’, and press enter.↩︎\nYou might want to look at this page: https://style.tidyverse.org/. I try to follow this style guide as much as possible.↩︎\nSee (https://adv-r.hadley.nz/names-values.html)↩︎\nWhile you are writing, you may find it occasionally useful to use the console to double check something. But all steps required to repeat your analysis should be in a script or markdown (more about markdown later).↩︎\n The same can be achieved by pressing the Run button at the top right. But since you are likely to be running code very frequently, it is best to learn the keyboard shortcut.↩︎\n The keyboard shortcut for this is Cmd + Shift + S (macOS) or Ctrl + Shift + S (Windows). From now on you can look up keyboard shortcuts by using Option + Shift + K (macOS) or Alt + Shift + K (Windows).↩︎\nMore on this in the next session.↩︎\nI first became aware of this package through a tutorial produced by Kevin Watson for LING316.↩︎\n For the rationale see: https://r4ds.hadley.nz/workflow-scripts.html.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "data_processing.html",
    "href": "data_processing.html",
    "title": "2  Data Processing",
    "section": "",
    "text": "2.1 Processing Data in Base R.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data_processing.html#processing-data-in-base-r.",
    "href": "data_processing.html#processing-data-in-base-r.",
    "title": "2  Data Processing",
    "section": "",
    "text": "2.1.1 Loading in Data\nUsually, you will load data from an external file. It is best practice to treat this file as ‘read only’ for the purposes of your R project. That is, you shouldn’t be overwriting the data file in the course of your project. Usually this data will be either:\n\na csv file: CSV stands for comma separated values. It is a text file in which values a separated by commas and each new line is a new row. It is an example of the more general class of ‘delimited’ files, where there is a special character separating values (another one you might come across is ‘tab separated values’ — where the values are separated by a tab).\nan Excel file (.xlsx or .xls). There is no base R function to directly read Excel files, but we have already loaded the readxl package which allows this. Be extra careful when loading Excel files. Excel may have modified the data in various ways. Famously, dates and times are difficult to manage. We will consider sanity checks in the following chapter.\n\nThere are other possibilities as well. Perhaps, for instance, you are collaborating with someone using SPSS or some other statistical software. There are too many cases to consider here, but Google will be your friend.\nHere, we’ll read a csv of reading from vowels from the QuakeBox corpus using the base R read.csv function. The most simple approach is:\n\nvowels &lt;- read.csv('data/qb_vowels.csv')\n\nI often use the here package to manage file paths. These can be a little finiky, especially when shifting between devices and operating systems. In an R project, the function here() starts from the root directory of the project. We then add arguments to the function containing the names of the directories and files that we want. The here version of the previous line of code is:\n\nvowels &lt;- read.csv(here('data', 'qb_vowels.csv'))\n\nLook in your file browser to make sure you understand where the file you are loading lives and how the path you enter, either using relative paths within an R project and/or using the here package, relates to the file.\nWe should always check a few entries to make sure the data is being read in correctly. The head() function is very useful.\n\nhead(vowels)\n\n# A tibble: 6 × 14\n  speaker     vowel F1_50 F2_50 participant_age_category participant_gender\n  &lt;chr&gt;       &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;             \n1 QB_NZ_F_281 GOOSE   427  2050 46-55                    F                 \n2 QB_NZ_F_281 DRESS   440  2320 46-55                    F                 \n3 QB_NZ_F_281 NURSE   434  1795 46-55                    F                 \n4 QB_NZ_F_281 KIT     554  2050 46-55                    F                 \n5 QB_NZ_F_281 LOT     530  1130 46-55                    F                 \n6 QB_NZ_F_281 START   851  1810 46-55                    F                 \n# ℹ 8 more variables: participant_nz_ethnic &lt;chr&gt;, word_freq &lt;int&gt;, word &lt;chr&gt;,\n#   time &lt;dbl&gt;, vowel_duration &lt;dbl&gt;, articulation_rate &lt;dbl&gt;,\n#   following_segment_category &lt;chr&gt;, amplitude &lt;dbl&gt;\n\n\nWe see a mix of numerical values and characters.\nAnother useful function is summary which provides some nice descriptive statistics.\n\nsummary(vowels)\n\n   speaker             vowel               F1_50            F2_50     \n Length:26331       Length:26331       Min.   : 208.0   Min.   : 473  \n Class :character   Class :character   1st Qu.: 404.0   1st Qu.:1401  \n Mode  :character   Mode  :character   Median : 473.0   Median :1730  \n                                       Mean   : 506.1   Mean   :1733  \n                                       3rd Qu.: 586.0   3rd Qu.:2094  \n                                       Max.   :1054.0   Max.   :2837  \n                                                                      \n participant_age_category participant_gender participant_nz_ethnic\n Length:26331             Length:26331       Length:26331         \n Class :character         Class :character   Class :character     \n Mode  :character         Mode  :character   Mode  :character     \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n   word_freq          word                time         vowel_duration   \n Min.   :     0   Length:26331       Min.   :   0.54   Min.   :0.03000  \n 1st Qu.:   688   Class :character   1st Qu.: 150.30   1st Qu.:0.05000  \n Median :  2859   Mode  :character   Median : 313.10   Median :0.08000  \n Mean   :  8204                      Mean   : 472.27   Mean   :0.08747  \n 3rd Qu.:  7540                      3rd Qu.: 578.62   3rd Qu.:0.11000  \n Max.   :111471                      Max.   :3352.78   Max.   :1.81000  \n NA's   :7                                                              \n articulation_rate following_segment_category   amplitude    \n Min.   :0.7949    Length:26331               Min.   :35.53  \n 1st Qu.:4.3143    Class :character           1st Qu.:61.94  \n Median :4.8843    Mode  :character           Median :66.61  \n Mean   :4.9352                               Mean   :66.34  \n 3rd Qu.:5.4927                               3rd Qu.:70.80  \n Max.   :8.8729                               Max.   :91.95  \n                                              NA's   :31     \n\n\nLook at the values for amplitude. Reading down the column we see that the minimum value for amplitude is \\(35.53\\), the first quartile is \\(61.94\\), and so on, until we read the entry NA's, which tells us that \\(31\\) entries in the column are missing.\nMany of the entries in this summary just say Class: character. Sometimes, we can get more information by turning a column into a factor variable. A factor variable is like a character variable, except that it also stores the range of possible values for the column. So, for instance, there is a short list of possible vowels in this data set. We can use the factor() function to create a factor variable and look at the resulting summary.\n\n# The use of `$` here will be explained in a moment.\nsummary(factor(vowels$vowel))\n\n  DRESS  FLEECE    FOOT   GOOSE     KIT     LOT   NURSE   START   STRUT THOUGHT \n   4596    3379     741    1454    3639    2428    1137    1272    3162    2012 \n   TRAP \n   2511 \n\n\nNow we see how many instances of each vowel we have in the data, rather than just Class: character.\nOf course, properly interpreting any of these columns requires subject knowledge and proper documentation of data sets! We will leave this aside for the moment as we work on the mechanics of data processing in R.\n\nAs always, check the documentation for a function which is new to you. Enter ?read.csv in the console.\n\nWhat is the default seperator between values for read.csv? \nWhich argument would you change if your csv does not have column names? \n\n\n\n\n2.1.2 Accessing Values in a Data Frame\nHow do we see what values are in a data frame? In RStudio, we can always click on the name of the data frame in the environment pane (recall: the environment pane is at the top right of the RStudio window). This will open the data as a tab in the source pane. It should look like a familiar spreadsheet programme, with the exception that you can’t modify the values.\nYou will very frequently see code that looks like this some_name$some_other_name. This allows us to access the object with the name some_other_name from an object with the name some_name. We’ve just loaded a big data frame. This data frame has a name (vowels — which you can see in the environment pane) and it has columns which also have names (for instance, participant_age_category). We can get access to these columns using $:\n\nword_frequencies &lt;- vowels$word_freq\n\nLook in the environment pane in the ‘Values’ section and you will see a new name (word_frequencies), followed by its type (‘int’ for ‘integer’ — numbers that don’t need a decimal point), how many values it has (\\(26331\\)) and the first few values in the vector. So the $ has taken a single column from the data frame we loaded earlier and we have assigned this to the variable word_frequencies). Enter the name word_frequencies in your console, and you will see all of the values from the word_frequencies vector.\nThere are a few ways to access values from this vector. We can use square brackets to get a vector which contains a subset of the original vector. If we wanted the first hundred elements from the vector, we would use square brackets and a colon:\n\nword_frequencies[1:100]\n\n  [1]     38   1606     97   5476   5151    845    797  34640    726   5879\n [11]   2405  24552   1233    556    347   2038   2175     63  24552     24\n [21]   4376  34640      0   8249   3080    762    383   6555    521     24\n [31]   2858   3080  29391   2858   9525  99820   9525   4376    521   3291\n [41]   5046     55    642   4376   6555    420      0    420  14049  12551\n [51]   5476  22666  21873   1340   5411   1492 111471    969     98    203\n [61]   2075   1147   1237   3299   2812   1237   4546   4135      0   5428\n [71]    785   1492  15724  11914    644   3371    644  11943  11943   3123\n [81]   1385   3123   5891    590   1078  24552    456    989   1381     78\n [91]  34640   1487   1487    688   1330      0      0    284      0      0\n\n\nThe colon produces a sequence from the number on the left to the number on the right, and the square brackets produce a subset of word_frequencies. We can put a single number inside square brackets. The result is a vector with a single value in it:\n\nword_frequencies[7]\n\n[1] 797\n\n\nWe can also use negative numbers to exclude the numbered elements. Here we exclude the values from 1 to 26231.\n\nword_frequencies[-1:-26231]\n\n  [1]  1625   136   136   128  4628   661    10  1938   244  2134  6515   424\n [13] 34640 14049  7749   750   892   384  2178  7749  2178   101  4161 26215\n [25]   428 16068   520  4843  5236  4135 11344  5843  7749 33749  4052     8\n [37]    42  6515  1611 10720    97   202   194 33749   164   164    97 43071\n [49] 11914    99  5236 22697  8880    37   518   243    10    56   892   384\n [61]  7749    10  1938  7540  3080  4135     0    37   750  3645  4100  4100\n [73]    29    19  3099  1581  3121  5242  4342  3524    29  1508  9931   358\n [85]   146 10720    97   911  4455  1223   153    56  3056   244    56   313\n [97] 34640  1638    56   711\n\n\nThis is equivalent to:\n\nword_frequencies[26232:26331]\n\n  [1]  1625   136   136   128  4628   661    10  1938   244  2134  6515   424\n [13] 34640 14049  7749   750   892   384  2178  7749  2178   101  4161 26215\n [25]   428 16068   520  4843  5236  4135 11344  5843  7749 33749  4052     8\n [37]    42  6515  1611 10720    97   202   194 33749   164   164    97 43071\n [49] 11914    99  5236 22697  8880    37   518   243    10    56   892   384\n [61]  7749    10  1938  7540  3080  4135     0    37   750  3645  4100  4100\n [73]    29    19  3099  1581  3121  5242  4342  3524    29  1508  9931   358\n [85]   146 10720    97   911  4455  1223   153    56  3056   244    56   313\n [97] 34640  1638    56   711\n\n\nThe use of the colon (:) creates a vector whose elements make up a numerical sequence. The vector we put inside the square brackets doesn’t need to be a sequence though. If we wanted the 3rd, 6th, 10th, and 750th entries in the vector we would say:\n\nword_frequencies[c(3, 6, 10, 750)]\n\n[1]   97  845 5879  644\n\n\nWe are again using c() to create a vector.\nYou can’t mix negative and positive numbers here:\n\nword_frequencies[c(3, 6, -10, 750)]\n\nError in word_frequencies[c(3, 6, -10, 750)]: only 0's may be mixed with negative subscripts\n\n\nIn this case, the error message is reasonably understandable.\nIn addition to numeric vector, we can subset with logical vectors. These are vectors which contain the values TRUE and FALSE. This is particularly important for filtering data. Let’s look at a simple example. We’ll create a vector of imagined participant ages and then create a logical vector which represents whether the participants are over 18 or not.\n\nparticipant_ages &lt;- c(10, 19, 44, 33, 2, 90, 4)\nparticipant_ages &gt; 18\n\n[1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n\n\nThe first participant is not older than \\(18\\), so their value is FALSE.\nNow, say we want the actual ages of those participants who are older than 18 we can combine the two lines above\n\nparticipant_ages[participant_ages &gt; 18]\n\n[1] 19 44 33 90\n\n\nIf you want a single element from a vector, then you use double square brackets ([[]]). So, for instance, the following will not work because it attempts to get two elements using double square brackets:\n\nparticipant_ages[[c(2, 3)]]\n\nError in participant_ages[[c(2, 3)]]: attempt to select more than one element in vectorIndex\n\n\nWhile this may seem like a very small difference, it can be a source of errors in practice. Some functions care about the difference between, say, a single number and a list containing a single number.\nWe can use square brackets with data frames too. The only differences comes from the fact that data frames are two dimensional whereas vectors are one dimensional. If we want the entry in the second row and the third column of the vowels data, we do this:\n\nvowels[2, 3]\n\n[1] 440\n\n\nWe can again use sequences or vectors. For instance:\n\nvowels[1:3, 4:6]\n\n# A tibble: 3 × 3\n  F2_50 participant_age_category participant_gender\n  &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;             \n1  2050 46-55                    F                 \n2  2320 46-55                    F                 \n3  1795 46-55                    F                 \n\n\nHere we get the values for the first three rows of the data frame from the fourth, fifth, and sixth columns.\nIf we want to specify just rows, or just columns, we can leave a blank space inside the square brackets:\n\n# rows only:\nvowels[1:3, ]\n\n# A tibble: 3 × 14\n  speaker     vowel F1_50 F2_50 participant_age_category participant_gender\n  &lt;chr&gt;       &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;             \n1 QB_NZ_F_281 GOOSE   427  2050 46-55                    F                 \n2 QB_NZ_F_281 DRESS   440  2320 46-55                    F                 \n3 QB_NZ_F_281 NURSE   434  1795 46-55                    F                 \n# ℹ 8 more variables: participant_nz_ethnic &lt;chr&gt;, word_freq &lt;int&gt;, word &lt;chr&gt;,\n#   time &lt;dbl&gt;, vowel_duration &lt;dbl&gt;, articulation_rate &lt;dbl&gt;,\n#   following_segment_category &lt;chr&gt;, amplitude &lt;dbl&gt;\n\n\n\n# columns only:\nvowels[, 1:3]\n\n# A tibble: 26,331 × 3\n   speaker     vowel F1_50\n   &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;\n 1 QB_NZ_F_281 GOOSE   427\n 2 QB_NZ_F_281 DRESS   440\n 3 QB_NZ_F_281 NURSE   434\n 4 QB_NZ_F_281 KIT     554\n 5 QB_NZ_F_281 LOT     530\n 6 QB_NZ_F_281 START   851\n 7 QB_NZ_F_281 DRESS   415\n 8 QB_NZ_F_281 STRUT   805\n 9 QB_NZ_F_281 START   857\n10 QB_NZ_F_281 TRAP    624\n# ℹ 26,321 more rows\n\n\nThe filtering code we looked at above is now a little more useful. What if we want to explore just the data from the female participants? We can use a logical vector again and the names of the columns.\n\nvowels_f &lt;- vowels[vowels$participant_gender == \"F\", ]\n\nWe have just filtered the entire data frame using the values of a single column. Look at the environment pain and you should now see two data frames: vowels and vowels_f. You should also see that one has many fewer rows than the other.\nNB: Don’t confuse == and =! The double == is used to test whether the left side and the right side are equal. The single = behaves like &lt;-. Here’s the kind of error you will see if you confused them:\n\n# This code is incorrect\nvowels_f &lt;- vowels[vowels$participant_gender = \"F\", ]\n\nError: &lt;text&gt;:2:46: unexpected '='\n1: # This code is incorrect\n2: vowels_f &lt;- vowels[vowels$participant_gender =\n                                                ^\n\n\nIt says unexpected '='. Usually this is a good sign that you should use ==.\nWe can also use names to filter. We know already that columns have names. We can give their names inside the square brackets. For instance:\n\nvowels_formants &lt;- vowels[, c(\"F1_50\", \"F2_50\")]\n\nThe above creates a data frame called vowels_formants containing all rows of the vowel data but only the columns with the names “F1_50” and “F2_50”.\nWith data frames, you can also just enter the names without the comma.\n\nvowels_formants &lt;- vowels[c(\"F1_50\", \"F2_50\")]\n\nThis code has the same effect as the previous code.\nTo get access to a single column, you can use a name with double square brackets. e.g.:\n\nword_frequencies &lt;- vowels[[\"word_freq\"]]\n\nIn fact, some_name$some_other_name is a shorthand form of some_name[[\"some_other_name\"]]. A single element of a list or vector returned by $ and [[]], whereas [] returns a list or vector which may contain multiple elements.1\n\nIf data_frame is a data frame, what will data_frame[5:6, ] return?\n\n This code will produce an error. The fifth and sixth row of the data frame The fifth and sixth columns of the data frame\n\nIf data_frame is a data frame, what will data_frame[, c(3, 6, 9)] return?\n\n This code will produce an error. The third, sixth, and ninth row of the data frame The third, sixth, and ninth column of the data frame.\n\nImagine you save a vector to the variable vector_name, as follows:\n\nvector_name &lt;- c(5, 2, 7, 3, 2)\n\nWhat would be the output of vector_name &gt; 2?\n\n [1]  TRUE FALSE  TRUE  TRUE FALSE TRUE FALSE [1] TRUE TRUE TRUE TRUE TRUE\n\nWhat would be the output of vector_name[3, 5]?\n\n [1] 7 2 TRUE Error in vector_name[3, 5] : incorrect number of dimensions\n\n\n\n\n2.1.3 Modifying Data\nWe’ve learned how to get access to data. But how to we change it? Usually this is as simple as an application of the &lt;-, which we have already seen.\nThe following block of code creates a new column in vowels which contains a log transformation of the word frequency column. Often this is a sensible thing to do with word frequency data.\n\nvowels$word_freq_ln &lt;- log(vowels$word_freq + 1)\n\nThis statement uses a few things we have already seen. Reading from the inside out, we see that the column with the name word_freq is being referenced from the vowels data frame (vowels$word_freq), and every element in the column is being increased by \\(1\\). Why? Well the logarithm of \\(0\\) is not defined and there are some \\(0\\)’s in this column. We take the log using the log() function (have a look at the documentation to see what the default behaviour of the function is). Finally, we use the &lt;- to put this data into a new column in the vowels data frame which we call word_freq_ln.\nWe can do this to individual elements as well. For instance, if we want to change the third entry in the participant_ages vector to \\(65\\), we would write participant_ages[3] &lt;- 65.\nIf we want to change the age of all participants with ages below 18 to \\(0\\), for whatever reason, we could say:\n\nparticipant_ages[participant_ages &lt; 18] &lt;- 0\n\nWe can also overwrite existing data in a data frame, including whole columns, this way.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data_processing.html#processing-data-in-the-tidyverse",
    "href": "data_processing.html#processing-data-in-the-tidyverse",
    "title": "2  Data Processing",
    "section": "2.2 Processing Data in the tidyverse",
    "text": "2.2 Processing Data in the tidyverse\nThere are (at least) two interacting dialects of R: the style associated with the tidyverse and the ‘base R’ approach. The tidyverse is a collection of packages which work well together and share a design philosophy. The most famous of these packages is ggplot, which implements a flexible approach to producing plots. This package will be the subject of a future workshop. We will focus instead on dplyr, a package which implements a set of ‘verbs’ for use with data frames. For more information see https://www.tidyverse.org/.\nMany scripts and markdowns will start with library(tidyverse), which loads all of the core tidyverse packages. To emphasise the modular nature of the tidyverse, this chapter only loads dplyr and readr.\nFor example, one verb is rename(). This function renames existing columns of a data frame. Another is mutate(). The mutate() function changes the data frame, typically by modifying existing columns or adding new ones. Moreover, these functions can be strung together in step-by-step chains of data manipulation using ‘pipes’. Here is an example of data processing in the tidyverse style using these functions:\n\nvowels &lt;- vowels %&gt;%\n  mutate(\n    word_freq_ln = log(word_freq + 1)\n  ) %&gt;%\n  rename(\n    F1_midpoint = F1_50,\n    F2_midpoint = F2_50\n  )\n\nThe pipe works by taking the object on the left of the pipe and feeding it to the function on the right side of the pipe as the first argument. So, e.g. 2 %&gt;% log() is the same as log(2). In this case, the data frame vowels becomes the first argument to mutate(), the following arguments then modify the data frame. Notice that we only need to say word_freq to refer to the column (rather than vowels$word_freq). This is because mutate() knows the names of the columns in the data frame it receives. Once the ‘mutation’ has happened, the modified data frame is pased to the rename function, which renames the columns F1_50 and F2_50 to F1_midpoint and F2_midpoint respectively. To see that this has happened, double click on vowels in the environment pane.\nThere are ongoing interactions between base R and the tidyverse. One particularly prominent instance is the inclusion of a ‘pipe’ operator in base R (|&gt;) which behaves in a very similar way to the tidyverse pipe (%&gt;%). I now prefer to use the base R pipe (|&gt;). In practice, I always use the shortcut ctrl + shift + M/command + shift + M to insert the pipe. RStudio has an option to choose whether the result of this is the tidyverse pipe or the base R pipe. I prefer to use the base R pipe now.\n\n\n\nSetting to use the base R pipe (|&gt;) or the magrittr pipe (%&gt;%). These options can be set globally with Tools &gt; Global Options or for a specific project with Tools &gt; Project Options.\n\n\nNB: you don’t need to entirely adopt either style! \n\n2.2.1 Reading in Data (again!)\nBefore we look in more detail at the data manipulation techniques which come with the dplyr package, we should look again at reading data. The readr package comes with modifications to the base R methods for reading in csv and similar files. readr includes the function read_csv (note the use of an underscore rather than dots, as in the base R function).\n\nvowels &lt;- read_csv(here('data', 'qb_vowels.csv'))\n\nRows: 26331 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): speaker, vowel, participant_age_category, participant_gender, parti...\ndbl (7): F1_50, F2_50, word_freq, time, vowel_duration, articulation_rate, a...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne great advantage of read_csv is that the output tells us something about the way R has interpreted each column of the data and the names which have been given to each column. We see how many rows and columns, what delimited the values (as we expected, \",\"), we also see what type of data R thinks these columns contain. So, chr means text strings. These are mostly participant metadata, such as their gender or ethnic background. We also see dbl columns. These contain numerical data.2\nAnother change between read.csv() and read_csv() is that the data is now a ‘tibble’ rather than a base R data frame. For current purposes, you don’t need to worry about this distinction. All of the base R methods we introduced above work with tibbles and with data frames.\nFor more information on importing data see the relevant chapter of R for Data Science.\n\n\n2.2.2 Some dplyr Verbs\nWhat are the main dplyr verbs?\nIf we want to filter a data frame, we use filter(). The first argument to filter() is a data frame. This argument is usually filled by the use of a pipe function. The remaining arguments are a series of logical statements which say which rows you want to keep and which you want to remove.\nWe can put multiple filtering statements inside the filter() function. Look at this code, which I will explain after the block:\n\nvowels_filtered &lt;- vowels |&gt; \n  filter(\n    following_segment_category == \"other\",\n    !is.na(amplitude),\n    between(F1_50, 300, 1000),\n    vowel_duration &gt; 0.01 | word_freq &lt; 1000\n  )\n\nThere are four statements being used to filter. Each is on a new line, but this is a stylistic choice, rather than one required for the code to work. You don’t need to use $ with column names inside tidyverse verbs.\n\nThe first statement uses ==. This says that we only want rows in the data frame where the following_segment column has the value other.\nThe second statement uses the function is.na() to test whether the amplitude column is empty or not. If a row has no value for amplitude then is.na() produces TRUE. So is.na(amplitude) will select the rows which have no value for amplitude. We want all the values which are not empty. So we add a ! to the start of the statement. This reverses the values so that TRUE becomes FALSE and vice versa. This statement now removes all the rows without amplitude information.\nThe third statement uses a helpful function from dplyr called between() which allows us to test whether a column’s value is between a two numbers. In this case, we want our values for the F1_50 column to be between \\(300\\) and \\(1000\\). This is inclusive. That is, it includes \\(300\\) and \\(1000\\). I forget this kind of thing all the time. To check, enter ?between in the console.\nThe fourth statement is made up of two smaller ones, combined with a bar (|). The bar means ‘or’. This will be TRUE when either of the statements is TRUE. So, in this case, it selects rows which have a vowel_duration value greater than \\(0.01\\) and rows which have a word_freq value less than \\(1000\\).3\n\nWe have seen mutate() and rename() already. mutate() is used to create new columns and to modify existing columns. The statements in a mutate() function are all of the form column_name =, with an R statement defining the values which the column will take. These can be either a single value, if you want every row of the column to have the same value (e.g. version = 1 would create a column called version which always has the value \\(1\\)), or a vector with a value for each row of the data frame. If you get this wrong it will look like this:\n\nvowels |&gt; \n  mutate(\n    bad_column = c(1, 2, 3, 4)\n  )\n\nError in `mutate()`:\nℹ In argument: `bad_column = c(1, 2, 3, 4)`.\nCaused by error:\n! `bad_column` must be size 26331 or 1, not 4.\n\n\nIf you want a subset of the columns, you can use select(). Here is an example:\n\nparticipant_metadata &lt;- vowels_filtered |&gt; \n  select(\n    speaker,\n    contains('participant_')\n  ) |&gt; \n  unique()\n\nThe select() verb here has two arguments. The first is just the name speaker. This, unsurprisingly, selects the column speaker. The second, contains('participant_') uses the function contains() (also from dplyr) to pick out columns containing the string 'participant_' in their names.4 After selecting these columns, there are many duplicate rows, so we pass the result into the base R function unique() with a pipe. The result is a data frame with the meta data for each participant in the data. Look in the environment pane to see how many rows and columns there are in the data frame participant_metadata.\nThe relocate() function is sometimes useful with the nzilbb.vowels package and other packages where the order of the columns is important. It relocates columns within a data frame. See for instance,\nWe have just covered:\n\nmutate(): the change a data frame.\nrename(): to change the names of existing columns.\nfilter(): to filter the rows of a data frame.\nselect(): to select a subset of columns of the data.\nrelocate(): to move selected columns within a data frame.\n\n\n\n2.2.3 Grouped Data\nAnother advantage of dplyr is that the same functions work for grouped data and ungrouped data. What is grouped data? It is data in which a group structure is defined by one or more of the columns.\nThis is most clear by example. In the data frame we have been looking at, we have a series of readings for a collection of speakers. There are only 77 speakers in the data frame, from which we get more than 20,000 rows. What if we want to get the mean values of these observations for each speaker? That is, we treat the data frame as one which is grouped by speaker.\nIn order to group data we use the group_by() function. In order to remove groups we use the ungroup() function. Let’s see this in action:\n\nvowels_filtered &lt;- vowels_filtered |&gt; \n  group_by(speaker) |&gt; \n  mutate(\n    mean_F1 = mean(F1_50),\n    mean_amplitude = mean(amplitude)\n  ) |&gt; \n  ungroup()\n\nThe above code groups the data, then uses mutate(), in the same way as we did above, to create two new columns. These use the mean() function, from base R, to calculate the mean value for each speaker. Have a look at the data, using one of the methods for accessing data we discussed above, to convince yourself that each speaker is given a different mean value. The same points apply to amplitude. Note, by the way, that the mean() function returns NA if there are any NA values in the column. This is a common issue. If you see NA when you expect a sensible mean, you can add na.rm = TRUE to the arguments of mean() or filter out any rows with missing information before you apply mean().\nSometimes we want summary information for each group. In this case, it is useful to have a data frame with a single row for each group. To do this, we use summarise rather than mutate. We can combine the output of the code block we have just looked at with the participant metadata as follows:\n\nvowels_summary &lt;- vowels_filtered |&gt; \n  group_by(speaker) |&gt; \n  summarise(\n    n = n(),\n    mean_F1 = mean(F1_50),\n    mean_amplitude = mean(amplitude),\n    gender = first(participant_gender),\n    age_category = first(participant_age_category)\n  )\n\nvowels_summary |&gt; \n  head()\n\n# A tibble: 6 × 6\n  speaker         n mean_F1 mean_amplitude gender age_category\n  &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       \n1 QB_NZ_F_138    86    572.           61.0 F      18-25       \n2 QB_NZ_F_161   273    433.           76.9 F      18-25       \n3 QB_NZ_F_169   100    461.           69.5 F      66-75       \n4 QB_NZ_F_195   138    558.           65.9 F      26-35       \n5 QB_NZ_F_200   259    521.           63.7 F      56-65       \n6 QB_NZ_F_213   218    513.           69.4 F      66-75       \n\n\nThe code above has two statements. We create a data frame called vowels_summary, which uses summarise() instead of mutate(). The second statement outputs the first six rows of vowels_summary using the head() function. Each row is for a different speaker and each column is defined by the grouping structure and summarise(). There is a column for each group (in this case just speaker), and then one for each argument to summarise(). The first, n, uses the n() function to count how many rows there are for each speaker. The second and third columns contain the mean value for the speaker for two variables. We then use the function first() to pull out the first value for the speaker for a given column. This is very useful in cases when every row for the speaker should have the same value. In this case, the speaker’s age category, for instance, does not change within a single recording so we can safely just take the first value for participant_age_category for each speaker.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data_processing.html#further-resources",
    "href": "data_processing.html#further-resources",
    "title": "2  Data Processing",
    "section": "2.3 Further Resources",
    "text": "2.3 Further Resources\n\nFor a fuller introduction to data transformation in the tidyverse see R for Data Science\nFor a discussion more focused on linguistics see the second chapter of Winter (2019).\nFor a discussion of the differences between base R and dplyr approaches to data processing see this vignette from dplyr.\n\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data_processing.html#footnotes",
    "href": "data_processing.html#footnotes",
    "title": "2  Data Processing",
    "section": "",
    "text": "There are some slight differences. If you are interested see the relevant section of Advanced R↩︎\nA dbl is a double length floating point number… Just think a number which can have a decimal point. Computers are, obviously, finite systems. Numbers are not (well… you could become a strict finitist I suppose). The technicalities of representing numbers on computers are very interesting, but we will avoid them where we can!↩︎\n This juggling of ‘true’ and ‘false’ is a bit of formal logic and make take a while to get your head around if you haven’t come across formal logic before.↩︎\n The documentation for select() covers a bunch of other helper functions like contains(). As usual, just type ?select in the console.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "rstudio_server.html",
    "href": "rstudio_server.html",
    "title": "3  RStudio Server",
    "section": "",
    "text": "Academics at the University of Canterbury can use the RStudio Server.\nAs of this writing writing… [details]\nCurrent details",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio Server</span>"
    ]
  },
  {
    "objectID": "qualtrics.html",
    "href": "qualtrics.html",
    "title": "4  Loading Data from Qualtrics",
    "section": "",
    "text": "4.1 Set up an R Project\nTo set up a new R project:\nWe will look at how to load data from .csv files generated by Qualtrics.\nPlace the .csv file you want to use in the data directory. For this exercise we will use this csv.\nCreate an R script and save it in the scripts directory.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#set-up-an-r-project",
    "href": "qualtrics.html#set-up-an-r-project",
    "title": "4  Loading Data from Qualtrics",
    "section": "",
    "text": "Go to File &gt; New Project\nSelect ‘New Directory’ and then ‘New Project’\nUse the browse choose a directory for the project (you might have to create a new one). For instance, Documents/linguistics_projects/ then enter a name for the project directory in the Directory name box.\nPress Create Project.\nCreate two directories in the project directory:\n\ndata\nscripts",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#load-useful-libraries",
    "href": "qualtrics.html#load-useful-libraries",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.2 Load Useful Libraries",
    "text": "4.2 Load Useful Libraries\nStart the script with the following lines:\n\nlibrary(tidyverse)\nlibrary(here)\n\nlibrary(ggcorrplot)\n\n# The following code changes the ggplot theme. You may like to explore\n# alternative themes.\ntheme_set(theme_bw())\n\nIf R says that a package is not found, install it using install.packages(). Simply enter the name of the package inside quotation marks inside the brackets. For instant, if you don’t have the ggcorrplot package, enter the following in the R console (the bottom pane in RStudio): install.packages('ggcorrplot')",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#load-data-exported-from-qualtrics-with-read_survey",
    "href": "qualtrics.html#load-data-exported-from-qualtrics-with-read_survey",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.3 Load Data Exported from Qualtrics with read_survey()",
    "text": "4.3 Load Data Exported from Qualtrics with read_survey()\nWe can now use the read_survey function from the qualtRics package to read in the data.\n\n# load the qualtRics package\nlibrary(qualtRics)\n\n# Read in the data\nsurvey &lt;- read_survey(here('data', 'Ling310-Week1-2022.csv')) \n\nRunning the above line of code will both read in the data (giving it the name survey in R) and provide output which tells us how the columns have been interpreted. The output shows that the default is to assume that each column has text in it rather than numbers (see .default = col_character() in the output). We also see that the Progress, Duration (in seconds), and latitude and longitude columns contain numbers (see col_double() in the output — ‘double’ refers to a way in which computer represent numbers which can contain a decimal point). Finally, there are a set of columns with the type col_logical(). These columns are ‘logical’ in the sense that they contain only TRUE or FALSE. Any of these columns can have NA, which indicates that data is missing or unavailable.\nYou may be confused now! Why would a column called RecipientLastName, for instance, only have the values TRUE or FALSE? Shouldn’t it have… the last names of recipients? There must be something wrong here. Let’s figure it out.\nIt is worth having a look at some rows from the columns we are worried about. There are only 17 rows in the data frame so we can easily look at all of the values in the confusing columns.1 Here is some tidyverse style code to do that:\n\nsurvey |&gt;\n  # Select the 'logical' columns which are confusing.\n  select(\n    RecipientLastName, \n    RecipientFirstName, \n    RecipientEmail, \n    ExternalReference\n  )\n\n# A tibble: 17 × 4\n   RecipientLastName RecipientFirstName RecipientEmail ExternalReference\n   &lt;lgl&gt;             &lt;lgl&gt;              &lt;lgl&gt;          &lt;lgl&gt;            \n 1 NA                NA                 NA             NA               \n 2 NA                NA                 NA             NA               \n 3 NA                NA                 NA             NA               \n 4 NA                NA                 NA             NA               \n 5 NA                NA                 NA             NA               \n 6 NA                NA                 NA             NA               \n 7 NA                NA                 NA             NA               \n 8 NA                NA                 NA             NA               \n 9 NA                NA                 NA             NA               \n10 NA                NA                 NA             NA               \n11 NA                NA                 NA             NA               \n12 NA                NA                 NA             NA               \n13 NA                NA                 NA             NA               \n14 NA                NA                 NA             NA               \n15 NA                NA                 NA             NA               \n16 NA                NA                 NA             NA               \n17 NA                NA                 NA             NA               \n\n\nAll 17 rows have NA values. That is, there is no data in these columns. We could just ignore these columns or we can get rid of them. Either option is fine. Here’s one way to get rid of them, using select() again, but this time with a minus sign (-) to indicate that we don’t want the named columns.\n\nsurvey &lt;- survey |&gt;\n  select(\n    -RecipientLastName, \n    -RecipientFirstName, \n    -RecipientEmail, \n    -ExternalReference\n  )\n\nWhat is actually in this data? Let’s look at the first few entries. If you simply enter survey we see the following:\n\nsurvey\n\n# A tibble: 17 × 36\n   StartDate       EndDate Progress Duration (in seconds…¹ Finished RecordedDate\n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;                  &lt;dbl&gt; &lt;lgl&gt;    &lt;chr&gt;       \n 1 18/07/2022 1:27 18/07/…      100                    339 TRUE     18/07/2022 …\n 2 19/07/2022 0:13 19/07/…      100                    570 TRUE     19/07/2022 …\n 3 19/07/2022 0:35 19/07/…      100                    251 TRUE     19/07/2022 …\n 4 19/07/2022 20:… 19/07/…      100                    277 TRUE     19/07/2022 …\n 5 19/07/2022 21:… 19/07/…      100                    265 TRUE     19/07/2022 …\n 6 20/07/2022 20:… 20/07/…      100                    343 TRUE     20/07/2022 …\n 7 20/07/2022 21:… 20/07/…      100                    390 TRUE     20/07/2022 …\n 8 20/07/2022 21:… 20/07/…      100                    328 TRUE     20/07/2022 …\n 9 21/07/2022 1:58 21/07/…      100                    213 TRUE     21/07/2022 …\n10 21/07/2022 17:… 21/07/…      100                    177 TRUE     21/07/2022 …\n11 21/07/2022 19:… 21/07/…      100                    497 TRUE     21/07/2022 …\n12 21/07/2022 19:… 21/07/…      100                    213 TRUE     21/07/2022 …\n13 21/07/2022 20:… 21/07/…      100                    203 TRUE     21/07/2022 …\n14 22/07/2022 22:… 22/07/…      100                    280 TRUE     22/07/2022 …\n15 24/07/2022 21:… 24/07/…      100                    639 TRUE     24/07/2022 …\n16 25/07/2022 19:… 25/07/…      100                     91 TRUE     25/07/2022 …\n17 28/07/2022 20:… 28/07/…      100                    159 TRUE     28/07/2022 …\n# ℹ abbreviated name: ¹​`Duration (in seconds)`\n# ℹ 30 more variables: ResponseId &lt;chr&gt;, LocationLatitude &lt;dbl&gt;,\n#   LocationLongitude &lt;dbl&gt;, DistributionChannel &lt;chr&gt;, UserLanguage &lt;chr&gt;,\n#   Q3 &lt;chr&gt;, Q4 &lt;chr&gt;, Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q8 &lt;chr&gt;, Q9 &lt;chr&gt;, Q10 &lt;chr&gt;,\n#   Q11 &lt;chr&gt;, Q12 &lt;chr&gt;, Q13 &lt;chr&gt;, Q14 &lt;chr&gt;, Q15 &lt;chr&gt;, Q16 &lt;chr&gt;,\n#   Q17 &lt;chr&gt;, Q18 &lt;chr&gt;, Q19 &lt;chr&gt;, Q20 &lt;chr&gt;, Q21 &lt;chr&gt;, Q22 &lt;chr&gt;,\n#   Q23 &lt;chr&gt;, Q24 &lt;chr&gt;, Q25 &lt;chr&gt;, Q26 &lt;chr&gt;, Q27 &lt;chr&gt;, Q28 &lt;chr&gt;\n\n\nThis doesn’t tell us much, because there are so many columns. How many? Look at the top left of the output: there are 17 rows and 40 columns (17 x 40). We see when the participant started and ended the survey, how far through they got (Progress), how long it took (`Duration (in seconds)`), and whether the participant finished (Finished).\nHave a look at the full data set in RStudio’s viewer by either clicking on the name survey in the environment pane (top right) or entering View(survey) in the console. There are a series of columns, one for each question. When we view in the RStudio viewer we see the text of survey question as ‘labels’ underneath the variable names (e.g. the variable named Q23 has the label ‘Does “pool” rhyme with “food”?’).\nThe other way to get at these labels is to use the function sjlabelled::get_label(). This will output the text label for each variable. The bit of code [15:20] tells R just to print the 30th label through to the 35th. You can change these numbers to see more labels or remove them entirely to see all of the labels.\n\nsjlabelled::get_label(survey)[30:35]\n\n                                                                                      Q22 \n                                                        \"Does 'pear' rhyme with 'share'?\" \n                                                                                      Q23 \n                                                     \"Does \\\"pool\\\" rhyme with \\\"food\\\"?\" \n                                                                                      Q24 \n                                             \"Are 'doll' and 'dole' pronounced the same?\" \n                                                                                      Q25 \n                \"Do you think Aucklanders sound different from people from Christchurch?\" \n                                                                                      Q26 \n\"Do you think you could guess whether someone is Māori or Pākeha from listening to them?\" \n                                                                                      Q27 \n              \"Would it be easier to guess someone's age or job from the way they sound?\" \n\n\nYou may need to scroll right to see the full question and the variable names in the output.\nThe bit of code sjlabelled:: means that we are looking for a name which exists within the package sjlabelled. You can avoid having to enter this in by loading the library sjlabelled at the start of your script. This is entirely up to you.\n\nWhat is the label of the variable named Q25?\n\n Does 'pear' rhyme with 'share'? Do you think Aucklanders sound different from people from Christchurch? It doesn't have a label.\n\n\nLet’s have a look at the actual answers for Q24:\n\nsurvey$Q24\n\n [1] \"No\"  \"Yes\" \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"No\" \n[13] \"No\"  \"Yes\" \"No\"  \"Yes\" \"Yes\"\nattr(,\"label\")\n                                         Q24 \n\"Are 'doll' and 'dole' pronounced the same?\" \n\n\nThe responses are stored as character strings. Most respondents seem to think that ‘doll’ and ‘dole’ are pronounced the same.\nWe don’t have to count these manually. Use the following code to see the counts of each answer:\n\nsummary(factor(survey$Q24))\n\n No Yes \n  7  10 \n\n\nThere are ten respondents who think ‘Aucklanders and people from Christchurch sound the same and four who don’t ’doll’ and ‘dole’ are pronounced the same.\n\nWhat do you think the function factor is doing here? See what happens if you remove it and have a look here to understand what is going on in more detail: https://r4ds.hadley.nz/factors.html",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#a-bar-chart",
    "href": "qualtrics.html#a-bar-chart",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.4 A Bar Chart",
    "text": "4.4 A Bar Chart\nLet’s create some plots in ggplot using the data we have loaded from Qualtrics.\n\nsurvey |&gt; \n  ggplot(\n    aes(\n      x = Q28\n    )\n  ) +\n  geom_bar() +\n  labs(\n    title = \"Q28\",\n    subtitle = \"What does the word 'worry' rhyme with?\"\n  )",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#factor-associations",
    "href": "qualtrics.html#factor-associations",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.5 Factor Associations",
    "text": "4.5 Factor Associations\nWe are often interested in associations between answers to distinct questions. The table function can be very useful as an initial was of seeing differences. Table is not a tidyverse function, and so we have to use the $ to indicate column names.\n\ntable(\n  survey$Q8,\n  survey$Q10\n)\n\n                \n                 No Yes\n  Definitely      5   2\n  Definitely not  0   3\n  Maybe?          3   4\n\n\n\nFigure out what the questions associated with the columns Q8 and Q10 are. Enough information has been given above to work this out.\n\nAre we surprised by the absence of ‘no’ answers in the ‘definitely not’ group? We can do a simple test to check this using a Chi-square test.\n\nsurvey_test &lt;- chisq.test(\n  survey$Q8,\n  survey$Q10\n)\n  \nsurvey_test\n\n\n    Pearson's Chi-squared test\n\ndata:  survey$Q8 and survey$Q10\nX-squared = 4.3849, df = 2, p-value = 0.1116\n\n\nHere, a value is calculated which compares the counts in the table above that we would expect given no association between the answers to the two questions, and what we actually observe. This value is called the \\(\\chi^2\\) value (or ‘Chi-square’). The consistency of our data with the assumption that there is no relationship is captured by the \\(\\chi^2\\) value (and the degrees of freedom, but you can ignore this for now). If the p-value is below a pre-established limit, typically \\(0.05\\), we say that the association between the two questions is ‘statistically significant’. In this case, the association is not statistically significant.\nThe assumption that there is no association between the questions is called the ‘null hypothesis’.\n\n\n\n\n\n\nWarning\n\n\n\nIt takes some effort to fully understand the meaning of p-values, and of the phrase ‘statistically significant’.\nFor one thing, ‘statistical significance’ does not mean significant in the sense of ‘big’. Some ‘statistically significant’ effects are so small that they have no practical importance.\nThis is a bigger topic which we will not cover here!\n\n\nWe can see what values were expected given no association between the questions here:\n\nsurvey_test$expected\n\n                survey$Q10\nsurvey$Q8              No      Yes\n  Definitely     3.294118 3.705882\n  Definitely not 1.411765 1.588235\n  Maybe?         3.294118 3.705882\n\n\nFailure to find a statistically significant effect does not mean that there is no effect. For instance, even if there is an effect we may not have enough data points to detect it.\nIt is also important to note that an association between two questions can appear as statistically significant even if it does not exist. The p-value indicates the probability that we would see a \\(\\chi^2\\) value this big (or bigger) in the absence of a genuine association between the answers to the two questions. Any data is consistent with the absence of an association. Perhaps we were just unlucky in our observations.\nTo report a \\(\\chi^2\\) test, you can say something like: &gt; We carried out a Chi-square test on questions 8 and 10 and failed to reject the null hypothesis (Chi = 4.4, df = 2, p = 0.1).\nOr, if the association were statistically significant: &gt; We found a statistically significant association between questions x and y (Chi = ???, df = ???, p = ???).\nWith the appropriate values inserted.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#wide-and-long-data",
    "href": "qualtrics.html#wide-and-long-data",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.6 Wide and Long Data",
    "text": "4.6 Wide and Long Data\nOne of the most common things to do with survey data is to convert from our current situation, where we have a column for each question to one in which we use two columns: one to identify questions and one to store responses.\nThat is, we want to move from a wider data frame to a longer data frame. A longer data frame as fewer columns and more rows.\nWe achieve this using the pivot_longer() function. It is often useful to assign the longer version of the data frame to a new variable.\nLet’s import another Qualtrics survey for which this is necessary:\n\nskills &lt;- read_survey(here('data', 'ARTS102-S2-2022.csv'))\n\n\nHave a look at this survey using the skills you have picked up above.\n\nHere is a long, but easier to explain, version of the code which makes this data frame longer:\n\nskills_longer &lt;- skills |&gt; \n  pivot_longer(\n    # First we name the columns we want to convert into longer formant.\n    cols = c(\n      Q18_1, Q18_2, Q18_3, Q18_4, Q18_5, Q18_6, Q18_7, Q18_8, Q18_9, \n      Q18_10, Q18_11, Q18_12, Q18_13, Q18_14\n    ),\n    # Where do we want the column names to go? This will be the name of our new\n    # column to identify the question.\n    names_to = \"question\",\n    # Where do we want the responses to go?\n    values_to = \"response\"\n  )\n\nLet’s look at some ways to make this code a little more convenient to write. There are lots of ways to specify the columns we are interested in rather than writing them all in by hand.\n\nWe can use a colon (:) to indicate a range of columns. This will use the order in which the columns appear in the data frame (the same as the order in the RStudio viewer or the order you get if you enter names(skills) into the console). So, in this case, we can use Q18_1:Q18_14:\n\n\nskills_longer &lt;- skills |&gt; \n  pivot_longer(\n    cols = Q18_1:Q18_14,\n    names_to = \"question\",\n    values_to = \"response\"\n  )\n\n\nYou can select columns using starts_with() or ends_with(), if all the columns you want have names starting with or ending with a given character. All the columns we want start with Q and none of the other columns start with Q, we we can use:\n\n\nsurvey_longer &lt;- survey |&gt; \n  pivot_longer(\n    cols = starts_with('Q'),\n    names_to = \"question\",\n    values_to = \"response\"\n  )\n\n\nWe can use contains() if we have a specific string which each column contains (in this case contains('Q') would work).\n\nThere are some other column selection options. If you want more detail, enter ?tidyselect::language into the console and look at the help file which appears.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#another-plot",
    "href": "qualtrics.html#another-plot",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.7 Another Plot",
    "text": "4.7 Another Plot\nLet’s produce a nice violin plot, which will show the distribution of scores across multiple questions:\n\nskills_v_plot &lt;- skills_longer |&gt; \n  ggplot(\n    aes(\n      x = question, \n      y = response\n    )\n  ) +\n  geom_violin(draw_quantiles = c(0.5)) +\n  labs(\n    title = \"Response to skills questions in LING310 Survey\"\n  )\n  \nskills_v_plot\n\n\n\n\n\n\n\n\nIn the above plot, the bars give the median value for the question. So, for instance, for the first question, the median value is somewhere between \\(37.5\\) and \\(50\\), whereas the median for the fourteenth question is somewhere between \\(12.5\\) and \\(25\\).\nThe labels on the \\(x\\)-axis are not very clear. Let’s fix this! There’s no low-labour way to do this. The names are taken from the column names which we got when we read in the data. So we can modify the skills data frame and then pivot it into longer form again. I get the names by looking at the output of the sjlabelled::get_label() function again.2 We will change the names of the columns using the rename() function.\n\nskills &lt;- skills |&gt; \n  rename(\n    maths = Q18_1,\n    skipping = Q18_2,\n    knitting = Q18_3,\n    minecraft = Q18_4,\n    driving = Q18_5,\n    conversation = Q18_6,\n    handwriting = Q18_7,\n    maps = Q18_8,\n    programming = Q18_9,\n    drawing = Q18_10,\n    lego = Q18_11,\n    threading_needle = Q18_12,\n    cooking = Q18_13,\n    te_reo = Q18_14,\n  )\n\nWe pivot again:\n\nskills_longer &lt;- skills |&gt; \n  pivot_longer(\n    cols = maths:te_reo,\n    names_to = \"question\",\n    values_to = \"response\"\n  )\n\nAnd plot again. We add some lines to rotate the labels so they are readable:\n\nskills_v_plot &lt;- skills_longer |&gt; \n  ggplot(\n    aes(\n      x = question, \n      y = response\n    )\n  ) +\n  geom_violin(draw_quantiles = c(0.5)) +\n  labs(\n    title = \"Response to skills questions in LING310 Survey\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle=90, vjust = 0.8)\n  )\n  \nskills_v_plot\n\n\n\n\n\n\n\n\nThere are many ways this could be improved. Have a look for some ggplot2 tutorials online.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#continuous-associations",
    "href": "qualtrics.html#continuous-associations",
    "title": "4  Loading Data from Qualtrics",
    "section": "4.8 Continuous Associations",
    "text": "4.8 Continuous Associations\nCan we look at how these skills are related to one another? Yes!\nFirst, let’s look at a correlation plot:\n\nggcorrplot(\n  cor(skills |&gt; select(maths:te_reo))\n)\n\n\n\n\n\n\n\n\nThe red blocks indicate positive associations (when one goes up, the other does as well). There is only one negative correlation here, between Minecraft and handwriting. This suggests (weakly!) that increased Minecraft skill comes with weakened handwriting.\nOn the stronger end, it looks like there is a strong positive correlation between knitting skill and the ability to thread a needle and between reading maps and driving.\nWe can test all three associations using the cor.test() function. First: between Minecraft and handwriting:\n\ncor.test(\n  skills$minecraft,\n  skills$handwriting\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  skills$minecraft and skills$handwriting\nt = -1.9122, df = 293, p-value = 0.05683\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.222396789  0.003220217\nsample estimates:\n       cor \n-0.1110185 \n\n\nThe p-value here is just above 0.05. This is below the cutoff, so we do not say that the association is ‘statistically significant’.\nLet’s look at the association between threading a needle and knitting:\n\ncor.test(\n  skills$knitting,\n  skills$threading_needle\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  skills$knitting and skills$threading_needle\nt = 7.1109, df = 293, p-value = 8.847e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2817856 0.4769410\nsample estimates:\n      cor \n0.3836382 \n\n\nThis is a much stronger correlation (\\(0.383...\\)) and has a much lower p-value. It is below \\(0.05\\), so we can say that the association between the questions is statistically significant. For instance, by saying “the correlation between the skill of threading a needle and knitting is statistically significant at the 0.05 level (Pearson’s cor = \\(0.38\\), p-value &lt; \\(0.001\\)).”3\nWhat about the association between driving a car and reading maps:\n\ncor.test(\n  skills$driving,\n  skills$maps\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  skills$driving and skills$maps\nt = 6.1072, df = 293, p-value = 3.216e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2306929 0.4335966\nsample estimates:\n      cor \n0.3360379 \n\n\nThe story here is very similar.\n\n\n\n\n\n\nWarning\n\n\n\nThere is much more to be said about statistical testing. In particular, in actual research it is very bad practice to start with a plot of the strength of a series of correlations, pick the strongest ones, and then do a statistical test on them. This can result in many false associations. Rather, you should have a hypothesis about what associations will be significant before you look at the data.\n\n\nLet’s have a look at what the association between threading a needle and knitting looks like with one final plot:\n\nskills |&gt; \n  ggplot(\n    aes(\n      x = threading_needle,\n      y = knitting\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\nBecause our values have to be between \\(0\\) and \\(100\\), there are ‘floor’ and ‘ceiling’ effects here. That is, many of our values sit at either 0 or 100. Nonetheless, the correlation coefficient gives us a tool for expressing the association between these variables.\nThere is much more to say about how to analyse data of this sort. However, this is enough to get started with.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "qualtrics.html#footnotes",
    "href": "qualtrics.html#footnotes",
    "title": "4  Loading Data from Qualtrics",
    "section": "",
    "text": "You can see how many rows there are in a data frame by looking at the ‘environment’ pane at the top right of the R Studio window. You can also use the function nrow(). In this case, you could enter nrow(survey) into your script or the console pane in RStudio.↩︎\nYou could use any method you like to do this, including looking at the original spreadsheet in Excel.↩︎\nHere, because the p-value is so small, we simply report that it is less than a very small number \\(0.0001\\). It is good practice not to just say \\(&lt; 0.05\\).↩︎",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "gamms_1.html",
    "href": "gamms_1.html",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "",
    "text": "5.1 Overview\nThis chapter is going to grow over three initial sessions. The rough plan is:\nWe will be using the following libraries:\n# The usual suspects\nlibrary(tidyverse)\nlibrary(here)\n\n# GAMM-specific libraries\nlibrary(mgcv)\nlibrary(itsadug)\nlibrary(gratia)\n\n# Non-essential. Used in my `bam` to determine how many CPU cores to use.\nlibrary(parallel)\n\n# NZILBB vowel package\n# If you do not have this use the following lines of code:\n# install.packages('remotes')\n# remotes::install_github('nzilbb/nzilbb_vowels')\nlibrary(nzilbb.vowels)\n\n# Set ggplot theme\ntheme_set(theme_bw())\nThis workshop is heavily indebted to the workshops put together by Márton Sóskuthy and Martijn Wieling. Links to these are provided in Section 5.7.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#overview",
    "href": "gamms_1.html#overview",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "",
    "text": "Introduction to the idea of GAMs and how to specify parametric and smooth terms.\nThe second ‘M’: we’ll add random effects, looking at random intercepts, slopes, and smooths.\nHandling auto-correlation. How to work out when this is an issue, and the available options for solving it.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#introduction-to-gams",
    "href": "gamms_1.html#introduction-to-gams",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "5.2 Introduction to GAMs",
    "text": "5.2 Introduction to GAMs\n\n5.2.1 Why?\nStraight lines have a lot of advantages. They can be completely specified by two numbers: how steep they are (the slope) and where they intersect the \\(y\\)-axis (the intercept). Fitting a straight line through a collection of points is just a matter of finding the optimum slope and intercept.\nBut sometimes straight lines aren’t enough. There are plenty of effects in nature which do not follow a straight line. There are plenty of examples of trajectories with non-linear behaviour in the study of language. For instance, consider the following trajectory for the price vowel from ONZE via (Sóskuthy, Hay, and Brand 2019).1\n\n\nTo view the code click here\n# Source: https://osf.io/74mza\n\n# Load all data (we will use the full set later)\nprice &lt;- read_rds(here('data', 'price_anon.rds'))\n\n# The dataset will be explained in full below.\n# Pull out a single trajectory.\nprice &lt;- price |&gt; \n  filter(id == \"price_58\") |&gt; \n  pivot_longer(\n    cols = f1:f2,\n    names_to = \"formant_type\",\n    values_to = \"formant_value\"\n  )\n\n# Plot it\nprice_plot &lt;- price |&gt; \n  ggplot(\n    aes(\n      x = time,\n      y = formant_value,\n      colour = formant_type\n    )\n  ) +\n  geom_point() +\n  labs(\n    colour = \"Formant type\",\n    y = \"Frequency (Hz)\",\n    x = \"Time (s)\"\n  )\n\nprice_plot\n\n\n\n\n\nF1 and F2 of a PRICE vowel.\n\n\n\n\nIf we try to fit this trajectory with straight lines, we get:\n\n\nTo view the code click here\nprice_plot +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nF1 and F2 of a PRICE vowel with linear model.\n\n\n\n\nWhat we want instead, is a way to fit a non-linear relationship. GA(M)Ms provide one flexible way of doing this. A simple GAM for this trajectory looks like this:\n\n\nTo view the code click here\nprice_plot +\n  geom_smooth(method = \"gam\", se = FALSE)\n\n\n\n\n\nF1 and F2 of a PRICE vowel with GAM model.\n\n\n\n\nThe same comments apply to trajectories taken from, e.g., tongue sensors or derived from video footage. They also apply at very different time scales. Consider GAMMs fit through full monologues (Wilson Black et al. 2023), or to formant values across the history of a dialect (Brand et al. 2021).\n\n\n\n\n\n\nNote\n\n\n\nYou may have used polynomial models in the past. These models add terms such as \\(x^2\\) and \\(x^3\\) to models (where previously, only \\(x\\) was included). You can do a lot with polynomial models, but they have some negative properties. For a brief and accessible explanation of the shortcomings of polynomial models compared to GAMMs see the motivating example section from the following Gavin Simpson video: https://youtu.be/Ukfvd8akfco?si=COncckzAvpqvfIfj&t=107. A key issue with polynomial models, especially when the \\(n\\) in \\(x^n\\) gets very large, is called Runge’s Phenomenon. At the edges of the range of the data, high order polynomials become very unstable.\nSóskuthy (2017) introduces polynomial models as, in effect, a simple kind of GAMM. This is different from thinking of GAMMs as an alternative to polynomial models. Both are defensible views, but you may get confused if you try to think them both at the same time!\n\n\n\n\n5.2.2 What?\nWe’ll start with a bit of mathematics. A linear model looks like this:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon \\] where the \\(\\beta\\)’s are the model coefficients, the \\(x\\)’s are the model predictors, and the \\(\\epsilon\\) is an error term. The \\(\\beta_0\\) term is the aforementioned intercept and the other \\(\\beta_n\\)’s are slopes. These slopes estimate the effect of the attached predictor. For any predictor, all we get is a single slope and the intercept — a straight line.\nGAMs replace each of the \\(\\beta_n x_n\\) terms with a function of \\(x_n\\). This function will usually be some kind of smooth. We’ll look at this visually in a moment. But, mathematically, the overall GAM model looks like this:\n\\[  y = \\beta_0 + f_1(x_1) + f_2(x_2) + \\ldots + f_n(x_n) + \\epsilon .\\] The parameters have been replaces by functions. Now, the nature of the relationship between a given predictor and \\(y\\) need not be a straight line.\nThe functions we choose attempt to balance smoothness and wiggliness. Wiggliness simply means deviation from a straight line. Smoothness indicates a lack of bumps. We are speaking intuitively, but this language is used by the mathematicians as well!.\nLet’s consider one class of smooth, and how it can be used to balance these two demands: splines. Here, a set of basis functions is summed together to create a smooth line through the data.\nThe basis functions for one common set of splines looks like this ( borrowing code from Gavin Simpson):\n\n\nTo view the code click here\n# Simulate 500 observations\nset.seed(1)\nN &lt;- 500\ndata &lt;- tibble(\n  x = runif(N),\n  ytrue = map_dbl(\n    x, \n    \\(x) {x^11 * (10 * (1 - x))^6 + ((10 * (10 * x)^3) * (1 - x)^10)}\n  ),\n  ycent = ytrue - mean(ytrue),\n  yobs  = ycent + rnorm(N, sd = 0.5)\n)\nk &lt;- 10\nknots &lt;- with(data, list(x = seq(min(x), max(x), length = k)))\nsm &lt;- smoothCon(s(x, k = k, bs = \"cr\"), data = data, knots = knots)[[1]]$X\ncolnames(sm) &lt;- levs &lt;- paste0(\"f\", seq_len(k))\nbasis &lt;- pivot_longer(cbind(sm, data), -(x:yobs), names_to = 'bf')\n\nbasis |&gt; \n  ggplot(\n    aes(\n      x = x, \n      y = value, \n      colour = bf\n    )\n  ) +\n  geom_line(lwd = 2, alpha = 0.5) +\n  guides(colour = FALSE) +\n  labs(x = 'x', y = 'b(x)')\n\n\n\n\n\n\n\n\nFigure 5.1: Cubic regression spline basis functions with 10 knots.\n\n\n\n\n\nEach distinct ‘basis function’ is in a different colour. We fit out actual data by multiplying the functions by appropriate coefficients (to change how high they are on the graph) and adding them together.\nThere are a number of knots in Figure 5.1. These are the points at which the functions a joined together (informally speaking) and at which we aim to ensure smoothness. For this set of basis functions, the knots are at (red points):\n\n\nTo view the code click here\n# Simulate 500 observations\nset.seed(1)\nN &lt;- 500\ndata &lt;- tibble(\n  x = runif(N),\n  ytrue = map_dbl(\n    x, \n    \\(x) {x^11 * (10 * (1 - x))^6 + ((10 * (10 * x)^3) * (1 - x)^10)}\n  ),\n  ycent = ytrue - mean(ytrue),\n  yobs  = ycent + rnorm(N, sd = 0.5)\n)\nk &lt;- 10\nknots &lt;- with(data, list(x = seq(min(x), max(x), length = k)))\nsm &lt;- smoothCon(s(x, k = k, bs = \"cr\"), data = data, knots = knots)[[1]]$X\ncolnames(sm) &lt;- levs &lt;- paste0(\"f\", seq_len(k))\nbasis &lt;- pivot_longer(cbind(sm, data), -(x:yobs), names_to = 'bf')\n\nbasis |&gt; \n  ggplot(\n    aes(\n      x = x, \n      y = value, \n      colour = bf\n    )\n  ) +\n  geom_line(lwd = 2, alpha = 0.5) +\n  geom_point(\n    inherit.aes = FALSE, \n    aes(\n      x = x\n    ),\n    y = 0,\n    colour = \"red\",\n    data = as_tibble(knots),\n    size = 5\n  ) +\n  guides(colour = FALSE) +\n  labs(x = 'x', y = 'b(x)')\n\n\n\n\n\n\n\n\nFigure 5.2: Cubic regression spline basis functions with 10 knots (red dots).\n\n\n\n\n\nAgain, borrowing code from Gavin Simpson, we can see what this looks like for our simulated data.\n\n\nTo view the code click here\nbeta &lt;- coef(lm(ycent ~ sm - 1, data = data))\nwtbasis &lt;- sweep(sm, 2L, beta, FUN = \"*\")\ncolnames(wtbasis) &lt;- colnames(sm) &lt;- paste0(\"F\", seq_len(k))\n## create stacked unweighted and weighted basis\nbasis &lt;- as_tibble(wtbasis) %&gt;%\n  mutate(\n    x = data$x,\n    spline_fit = pmap_dbl(\n      # Yikes, bad coding here by me (JWB)\n      list(F1, F2, F3, F4, F5, F6, F7, F8, F9, F10),\n      sum\n    )\n  ) \n\nbasis_long &lt;- basis |&gt; \n  pivot_longer(\n    cols = contains('F'),\n    values_to = \"value\",\n    names_to = \"bf\"\n  )\n\n\ndata |&gt; \n  ggplot(\n    aes(\n      x = x,\n      y = yobs\n    )\n  ) +\n  geom_point(alpha = 0.4) +\n  geom_line(\n    aes(\n      x = x,\n      y = value,\n      colour = bf\n    ),\n    data = basis_long,\n    inherit.aes = FALSE,\n    linewidth = 1\n  ) +\n  geom_line(\n    aes(\n      x = x,\n      y = spline_fit\n    ),\n    inherit.aes = FALSE,\n    colour = \"black\",\n    linewidth = 1,\n    data = basis\n  ) + \n  guides(colour = FALSE)\n\n\n\n\n\n\n\n\nFigure 5.3: Simulated data (black dots) with the basis functions after multiplication by their weights (colourful lines) and the sum of the basis functions (black line).\n\n\n\n\n\nSpend some time looking at Figure 5.3. Convince yourself that if you added together the colourful lines you would get the black line. The easiest way to do this is to work one point on the \\(x\\)-axis at a time. The case where \\(x=0\\) is the easiest, where the only colourful line is the red one and it is at the same point on the \\(y\\)-axis as the black line.\nWhat about this wiggliness and smoothness trade off? We’ve already seen one way in which wiggliness can be controlled: the number of knots sets an upper limit on how wiggly the resulting smooth function can be. If we only had 3 knots, this is what we would get:\n\n\nTo view the code click here\nk &lt;- 3\nknots &lt;- with(data, list(x = seq(min(x), max(x), length = k)))\nsm &lt;- smoothCon(s(x, k = k, bs = \"cr\"), data = data, knots = knots)[[1]]$X\ncolnames(sm) &lt;- levs &lt;- paste0(\"f\", seq_len(k))\nbeta &lt;- coef(lm(ycent ~ sm - 1, data = data))\nwtbasis &lt;- sweep(sm, 2L, beta, FUN = \"*\")\ncolnames(wtbasis) &lt;- colnames(sm) &lt;- paste0(\"F\", seq_len(k))\n## create stacked unweighted and weighted basis\nbasis &lt;- as_tibble(wtbasis) %&gt;%\n  mutate(\n    x = data$x,\n    spline_fit = pmap_dbl(\n      # Yikes, bad coding here by me (JWB)\n      list(F1, F2, F3),\n      sum\n    )\n  ) \n\nbasis_long &lt;- basis |&gt; \n  pivot_longer(\n    cols = contains('F'),\n    values_to = \"value\",\n    names_to = \"bf\"\n  )\n\n\ndata |&gt; \n  ggplot(\n    aes(\n      x = x,\n      y = yobs\n    )\n  ) +\n  geom_point(alpha = 0.4) +\n  geom_line(\n    aes(\n      x = x,\n      y = value,\n      colour = bf\n    ),\n    data = basis_long,\n    inherit.aes = FALSE,\n    linewidth = 1\n  ) +\n  geom_line(\n    aes(\n      x = x,\n      y = spline_fit\n    ),\n    inherit.aes = FALSE,\n    colour = \"black\",\n    linewidth = 1,\n    data = basis\n  ) + \n  guides(colour = FALSE)\n\n\n\n\n\n\n\n\nFigure 5.4: Simulated data (black dots) with the basis functions after multiplication by their weights (colourful lines) and the sum of the basis functions (black line).\n\n\n\n\n\nThe black line is our best possible fit to the data here, but it is no good. It needs to be wigglier.\nSo knots are one determinant of wiggliness. But there is another: the smoothing parameter. This is used in order to penalise wiggliness when we fit a GAM model and is handled automatically by the mgcv package. In practice, it is determined from the data, rather than being manually specified. However, it is worth looking manually at what happens if we set the smoothing parameter too low and fail to sufficiently penalise wiggliness.\nHere’s what an excessively wiggly smooth function looks like with the New Zealand English dress vowel in ONZE:\n\n\nTo view the code click here\nonze_vowels_full |&gt; \n  lobanov_2() |&gt; \n  filter(\n    gender == \"F\",\n    vowel == \"DRESS\"\n  ) |&gt; \n  group_by(speaker) |&gt; \n  summarise(\n    F1_lob2 = mean(F1_lob2),\n    yob = first(yob)\n  ) |&gt; \n  ggplot(\n    aes(\n      x = yob,\n      y = F1_lob2\n    )\n  ) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(\n    method = \"gam\", \n    formula =  y ~ s(x, bs = \"cs\", k = 50, sp=0.01), \n    se=FALSE\n  )\n\n\n\n\n\n\n\n\nFigure 5.5: Mean normalised F1 for female speakers in ONZE by year of birth. Smoothing parameter set to 0.01.\n\n\n\n\n\nOn the other side, we can set the smoothing parameter too high. If we do, we’ll end up with a straight line.\n\n\nTo view the code click here\nmean_onze_full &lt;- onze_vowels_full |&gt; \n  lobanov_2() |&gt; \n  filter(\n    vowel == \"DRESS\"\n  ) |&gt; \n  group_by(speaker) |&gt; \n  summarise(\n    F1_lob2 = mean(F1_lob2),\n    yob = first(yob),\n    speech_rate = mean(speech_rate),\n    gender = first(gender)\n  )\n\nmean_onze_full |&gt; \n  filter(gender == \"F\") |&gt; \n  ggplot(\n    aes(\n      x = yob,\n      y = F1_lob2\n    )\n  ) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(\n    method = \"gam\", \n    formula =  y ~ s(x, bs = \"cs\", k = 50, sp=10000000), \n    se=FALSE\n  )\n\n\n\n\n\n\n\n\nFigure 5.6: Mean normalised F1 for female speakers in ONZE by year of birth. Smoothing parameter set to 10,000,000.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe are in the broad area of a core data science concept: the ‘bias variance tradeoff’. That is, with any statistical learning method, we can introduce errors with the assumptions of our model (bias) and errors due to excessively following small fluctuations in our data (variance). But the less bias we include, the more we will be lead astray by noise and vice versa. It’s a tradeoff.\nIn the case we are looking at now, reducing the smoothing parameter is equivalent to decreasing bias and increasing variance.\nSee the Wikipedia page.\n\n\n\n\n5.2.3 Fitting GAMs with mgcv\nHow do we specify GAMs with the mgcv package? Let’s start with model formulae.\nThe most obvious this is the construction of smooth terms. These use the s function. Let’s look at the ONZE data from Figure 5.5 and Figure 5.6. Here we want normalised first formant values to vary with year of birth in a non-linear way. We want a smooth on year of birth.\nWhat are the column names here?\n\nmean_onze_full\n\n# A tibble: 481 × 5\n   speaker  F1_lob2   yob speech_rate gender\n   &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;fct&gt; \n 1 CC_f_007  -0.765  1982        5.76 F     \n 2 CC_f_010  -0.554  1947        5.06 F     \n 3 CC_f_020  -0.242  1936        5.04 F     \n 4 CC_f_024  -0.760  1973        5.61 F     \n 5 CC_f_025  -0.747  1949        4.63 F     \n 6 CC_f_027  -0.962  1981        5.22 F     \n 7 CC_f_033  -0.725  1953        4.82 F     \n 8 CC_f_040  -0.731  1955        4.24 F     \n 9 CC_f_051  -0.769  1955        4.67 F     \n10 CC_f_052  -0.838  1942        5.61 F     \n# ℹ 471 more rows\n\n\nThe variable we want to explain is F1_lob2. This contains normalised mean first formant values for each speaker. We want to explain it using `yob’, which we can see from the tibble output, is an integer. We’ll look at incorporating the other variables later.\nThe simplest version of the formula is this: F1_lob2 ~ s(yob). But this is usually bad practice — we should be more explicit! As a general principle, relying on defaults is dangerous as they can change under you, causing your code to have a different outcome.\nThe first area to be explicit is the knots. The default value for many smoothing splines is \\(10\\) and this is almost always fine. But we should think about it each time we fit a GAMM. So, an improvement: F1_lob2 ~ s(yob, k = 10).\nThe second argument to highlight is bs. This says what kind of basis functions we are using. The default, tp, or thin plate regression splines, are fine. Typically this choice won’t make a big different to you. But I will add more detail here soon. Regardless, it is good to make this explicit. So our final version of this formula: F1_lob2 ~ s(yob, k = 10, bs = \"tp\").\nWhat do we do with this formula? We will use the function bam to fit our first GAM.\n\n\n\n\n\n\nNote\n\n\n\nWe could just as easily use the gam function, but bam is optimised for large datasets.\n\n\n\nonze_fit &lt;- bam(\n  formula = F1_lob2 ~ s(yob, k = 10, bs = \"tp\"),\n  data = mean_onze_full\n)\n\nWe obtain a summary for this model using the summary function:\n\nsummary(onze_fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nF1_lob2 ~ s(yob, k = 10, bs = \"tp\")\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.590123   0.008102  -72.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df     F p-value    \ns(yob) 3.633  4.481 178.6  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.625   Deviance explained = 62.8%\nfREML = -138.75  Scale est. = 0.031571  n = 481\n\n\nThis summary has two primary sections. The Parametric coefficients, which indicate the non-smooth aspects of the model. In this case, the model fits an intercept term, which sets the overall height of the smooth function. The Approximate significance of smooth terms section indicates, as it says, the approximate significance of our smooths. This is the GAM equivalent of the coefficient for a variable in a linear model.\nIn this model, we only have s(yob) to look at. We see that it has an edf or ‘estimated degrees of freedom’ of 3.633. This is an indication of how wiggly the line is. If the esimated degrees of freedom are 1, it’s pretty much a straight line. We also see a p-value entry. This indicates whether the shape of the smooth is statistically significantly different from a flat line at the intercept value. In this case, unsurprisingly, it is distinct from a flat line.\nBut there are some problems here. First, we are merging male and female data together here. What if we want to fit a smooth for both male and female speakers? Here we can us the by argument to s() and add a parametric term for gender. This results in:\n\nonze_fit_gender &lt;- bam(\n  formula = F1_lob2 ~ gender + s(yob, by = gender, k = 10, bs = \"tp\"),\n  data = mean_onze_full\n)\n\nsummary(onze_fit_gender)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nF1_lob2 ~ gender + s(yob, by = gender, k = 10, bs = \"tp\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.64493    0.01146 -56.255  &lt; 2e-16 ***\ngenderM      0.10157    0.01565   6.492 2.13e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df      F p-value    \ns(yob):genderF 2.754  3.394  72.13  &lt;2e-16 ***\ns(yob):genderM 1.000  1.000 588.35  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.658   Deviance explained = 66.2%\nfREML = -156.83  Scale est. = 0.028773  n = 481\n\n\nNow we have two intercept terms, one for the female speakers and one for the male, both of which are significant. Just as in generalised linear models, the genderM parametric term gives a difference from the female intercept. This indicates that the first formant value is on average higher for male speakers. In the smooth terms section we now see s(yob):genderF and s(yob):genderM. We get independent p-values for each. What we do not get is a representation of the difference between the smooth for the female speakers and the smooth for the male speakers.2\nNote that the smooth for the male speakers in effectively a straight line. We will see this in a moment when we visualise.\nWe will add one more thing to this model before turning to diagnostics and plotting. What if we want two smooths? We know that speech rate can affect formant values. We can add this as an additional smooth term as follows:\n\nonze_fit_rate &lt;- bam(\n  formula = F1_lob2 ~ gender + \n    s(yob, by = gender, k = 10, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\"),\n  data = mean_onze_full\n)\n\nsummary(onze_fit_rate)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nF1_lob2 ~ gender + s(yob, by = gender, k = 10, bs = \"tp\") + s(speech_rate, \n    k = 10, bs = \"tp\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.64505    0.01148 -56.176  &lt; 2e-16 ***\ngenderM      0.10180    0.01568   6.492 2.14e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df       F p-value    \ns(yob):genderF 2.754  3.393  64.128  &lt;2e-16 ***\ns(yob):genderM 1.000  1.000 488.583  &lt;2e-16 ***\ns(speech_rate) 1.000  1.000   0.082   0.775    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.658   Deviance explained = 66.2%\nfREML = -153.06  Scale est. = 0.028828  n = 481\n\n\nIn this case we don’t see a significant difference as a result of speech rate. This may be because we are working with mean values for the formants. Once we can include random effects, and thus multiple values from a single speaker, this will change!\n\n\n\n\n\n\nTip\n\n\n\nWieling (2018) provides an example of building up a model from the ground up, exploring many different possible structures and including the R code. It is a good first port of call for looking at additional possible structures.\n\n\n\n\n5.2.4 Model diagnostics\nThe primary port of call for model diagnostics in mgcv is the gam.check() function. One of the outputs is a text output to help determine if \\(k\\) is too low.\n\ngam.check(onze_fit_rate)\n\n\nMethod: fREML   Optimizer: perf newton\nfull convergence after 15 iterations.\nGradient range [-1.027586e-06,1.027607e-07]\n(score -153.0627 & scale 0.02882816).\nHessian positive definite, eigenvalue range [2.671181e-07,238.0032].\nModel rank =  29 / 29 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                 k'  edf k-index p-value  \ns(yob):genderF 9.00 2.75    1.00   0.530  \ns(yob):genderM 9.00 1.00    1.00   0.460  \ns(speech_rate) 9.00 1.00    0.93   0.065 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output above has an entry for each smooth term in the model. k' indicate the knots the smooth function has available. This, as explained above, is an upper limit on wiggliness. It will usually be one less than the value of k given in the model formula. The value edf indicates the actual wiggliness of the smooth. To determine if there is evidence of insufficient k, check whether the p-value is low and the edf is close to k'. If so, consider increasing k in the model. In this case, k is plenty high enough.\nLet’s see a case where this doesn’t work well.\n\nsim_fit &lt;- bam(\n  formula = yobs ~ s(x, k = 3, bs = \"cr\"),\n  data = data\n)\n\ngam.check(sim_fit)\n\n\nMethod: fREML   Optimizer: perf newton\nfull convergence after 11 iterations.\nGradient range [-2.519727e-09,2.506653e-09]\n(score 1287.688 & scale 9.969049).\nHessian positive definite, eigenvalue range [0.4972726,249.001].\nModel rank =  3 / 3 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n     k' edf k-index p-value    \ns(x)  2   2    0.03  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere k' and edf are the same, and the p-value is very low. Something is going wrong here. In fact, the problem here is the same problem we saw in Figure 5.4.\nIf you run gam.check you will also see some diagnostic plots. I have suppressed them in this document. The gratia package provides a nice wrapper for the gam.check visualisations (via the appraise function). It has the advantage of being a single plot in ggplot format which will take on any global changes to you ‘theme’. That is, it will produce output which matches your other plots.\nLet’s look at the bad fit to our simulated data again:\n\nappraise(sim_fit)\n\n\n\n\n\n\n\n\nThese plots tell us something about the residuals. The assumption of our model is that these residuals will be normally distributed. That is, we assume that the variation which is left behind by our model looks like random observations from a normal distribution.\nThese diagnostic plots are not like this. At the top left, we should see a straight line of black points following the red line. However, we see that at the tails, at extreme values of a predictor, we are not getting what we would expect from a normal distribution. The histogram tells a similar story. This should look like a nice(ish) bell curve. But the most extreme warning signs are the two plots on the right. These show the model predictions plotted against the actual values. There should be no obvious (non-linear) pattern in these plots.\nWe already know what to do in this case, we need to increase \\(k\\)! If we do, this is what we get:\n\nsim_fit_highk &lt;- bam(\n  formula = yobs ~ s(x, k = 20, bs = \"cr\"),\n  data = data\n)\n\nappraise(sim_fit_highk)  \n\n\n\n\n\n\n\n\nMuch better! And the check of k looks OK too:\n\ngam.check(sim_fit_highk)\n\n\nMethod: fREML   Optimizer: perf newton\nfull convergence after 5 iterations.\nGradient range [-1.018439e-06,1.032479e-06]\n(score 434.2567 & scale 0.2813964).\nHessian positive definite, eigenvalue range [8.433327,249.2893].\nModel rank =  20 / 20 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n       k'  edf k-index p-value\ns(x) 19.0 17.7     1.1    0.98\n\n\nReturning to our model of the ONZE data, let’s use appraise again:\n\nappraise(onze_fit_rate)\n\n\n\n\n\n\n\n\nThis looks basically fine. But we can see heavier tails that we would usually want at either end of the QQ plot and the histogram. This is quite common in vocalic data. One way to handle this is to assume that the residuals follow a t distribution instead (these have fatter tails than normal distributions). We can do this with the family argument to bam. If we do this, the plots look a bit better:\n\nonze_fit_rate &lt;- bam(\n  formula = F1_lob2 ~ gender + \n    s(yob, by = gender, k = 10, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\"),\n  data = mean_onze_full,\n  family = scat(link=\"identity\")\n)\n\nappraise(onze_fit_rate)\n\n\n\n\n\n\n\n\nWe will see a case below where a t-distribution functions better than a standard normal distribution with formant data. It is worth checking this case-by-case though.\n\n\n5.2.5 Plotting\nPlotting smooths can be done in at least three ways:\n\nUsing a prediction function to generate predictions from the model and then plot them yourself. The advantage is high flexibility in your plots.\nUse the plot_smooth() and related functions from itsadug.\nUse the GAM plotting functions from gratia.\n\nLet’s look at the plot_smooth() function. This has been used in a lot of projects at NZILBB.\n\nplot_smooth(\n  x = # the model,\n  view = # the name of the variable you want to plot.\n  cond = # a named list containing the values of other terms in the model. \n  # if not given you will get mean values.\n  plot_all = # The name of any factors which for which you want all levels to be\n  # plotted\n  rug = # display a 'rug' at the bottom of the plot to indicate where there are\n  # actual observations.\n)\n\nNow, using these to plot the ONZE model, we get:\n\nplot_smooth(\n  x = onze_fit_rate,\n  view = \"yob\",\n  plot_all = \"gender\",\n  rug = TRUE\n)\n\nSummary:\n    * gender : factor; set to the value(s): F, M. \n    * yob : numeric predictor; with 30 values ranging from 1864.000000 to 1982.000000. \n    * speech_rate : numeric predictor; set to the value(s): 4.86001666666667. \n    * NOTE : No random effects in the model to cancel.\n \n\n\n\n\n\n\n\n\n\nNote that the values given for other predictors are given in console output.\nNow change the view and cond arguments and see what happens.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#the-second-m-mixed-effects",
    "href": "gamms_1.html#the-second-m-mixed-effects",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "5.3 The Second ‘M’: Mixed Effects",
    "text": "5.3 The Second ‘M’: Mixed Effects\n\n\n\n\n\n\nWarning\n\n\n\nThe mgcv package requires the openmp library to take advantage of the parallel processing available through bam(). This is typically not a problem for Windows and Linux installations of R. On macos, it can be a problem.\nIf you get a warning about openmp not being available on macos. I suggest you install the mandatory tools listed here: https://mac.r-project.org/tools/ and then follow the instructions at https://mac.r-project.org/openmp/.\nThis is not likely to be a pleasant experience, especially if you don’t like battling with the command line. UC and NZILBB researchers and students, feel free to message me on Rocket.Chat or via joshua.black@canterbury.ac.nz and I may be able to help.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#auto-correlation",
    "href": "gamms_1.html#auto-correlation",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "5.4 Auto-correlation",
    "text": "5.4 Auto-correlation",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#hypothesis-testing",
    "href": "gamms_1.html#hypothesis-testing",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "5.5 Hypothesis Testing",
    "text": "5.5 Hypothesis Testing\n\n\n5.5.1 Visual methods\n\n\n5.5.2 Model summary\n\n\n5.5.3 itsadug::compareML().",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#reporting-a-gamm",
    "href": "gamms_1.html#reporting-a-gamm",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "5.6 Reporting a GAMM",
    "text": "5.6 Reporting a GAMM\nIt is important to know how to report a GAMM in a paper.\nThis section presents some (somewhat randomly selected) examples of GAMMs ‘in the wild’. All of these are good enough for publication. I will then give a few opinions.\n\nRenwick et al. (2023)Brand et al. (2021)Derrick and Gick (2021)\n\n\nHow did you fit the GAMMs?:\n\nWe built a separate GAMM for each combination of allophone, gender, and formant, leading to twenty-eight separate models. The dependent variable was log-means normalized formant values, from five measurement points per token. The models’ independent variables included a nonlinear, continuous smooth term for speaker year of birth (YoB), and a smooth term for measurement point (percent), ordered 20% to 80%. Smooths used four knots. The two smooths were combined into a tensor-product interaction allowing the predicted trajectory to freely vary in shape across YoB. Vowel duration was included as a parametric effect. The random effects structure comprised linear random intercepts and slopes for speaker, word, and collection. Model specifications and summaries appear in Appendix 1. Models were fitted using the mgcv::bam() function in R (Wood, 2017b). (Renwick et al. 2023, 185)\n\nAppendix 1 contains the output of the summary() function for each GAMM model pasted in to a word document. This includes the model formulae.\nHow did you evaluate the GAMMs?\n\nWe evaluate the GAMMs in two ways: first, via visualizations of predicted measurements, which were extracted from each model. Second, we tested the significance of YoB via model comparison (Renwick & Stanley, 2020; Stanley et al., 2021). For each GAMM, we constructed a model that excluded YoB but was otherwise identical. Each “dropped” model was evaluated against its “full” equivalent via itsadug:: compareML() (van Rij, Wieling, Baayen, & van Rijn, 2017), which returns a score and p-value through chi-squared testing of log-likelihood. If the “full” model including birth year is deemed to be significantly better ( p &lt; 0.05) than the “dropped” version without birth year, we infer that the shape and/or position of that vowel’s trajectory is meaningfully predicted by speaker year of birth. (Renwick et al. 2023, 185)\n\nHow did you report the results?\n\nModel comparisons confirmed that for all models save one (F2 of PRY, for women) the inclusion of YoB provided a significant improvement in fit over an identical GAMM lacking that term. For all allophones (including women’s pry, whose F1 model is improved by YoB), there is significant variation in trajectory shape and/or vowel space position across time. (Renwick et al. 2023, 186)\n\nA footnote indicates that pry has the lowest token count and the prediction that increased tokens would lead to an improvement by adding their year of birth term.\nA series of figures are then described in detail. For instance:\n\n\n\nFigure 5 from Renwick et al. 2023, p. 189.\n\n\nThe description is as follows:\n\nYoung Georgians’ trajectories are modeled in Figure 5, including birth years 1974 (left) and 2000 (right), representing Gen X and Gen Z. The vowel systems shown here are markedly different from older speakers’. For all groups, the /aɪ/ allophones’ offset has a lower F1 than bat, indicating its trajectory lengthening. bought remains slightly backer than bot, although they have similar ingliding trajectories. The lax front vowels have shifted. bat is ingliding and retracted, while bet is lower in Gen Z compared to Gen X. bait is very peripheral, especially in Gen Z, compared to the other front vowels, with a lower F1 and higher F2 throughout its trajectory. bait and bet are very distinct, and their trajectories do not cross; there is no face/dress swapping for these speakers. These effects are stronger for Gen Z than for Gen X. The picture shown for late twentieth century Georgians is not the SVS; instead, the Gen X speakers show a retreat from the SVS, characterized by retracting front lax vowels, which is consistent with the LBMS among younger, Gen Z speakers. [Renwick et al. (2023), 187\n\n\n\nAs in the Renwick et al. (2023) example, Brand et al. (2021) reports many GAMMs fit to readings from vowels. There is significantly more detail concerning the GAMMs in supplementary material for this paper hosted on osf.io and github.com. There is less detail in the paper itself, because the GAMMs are one stage in a larger method.\nHow did you fit the GAMMs?\nIn the paper the use of GAMMs is described as follows in the methodology section:\n\nSpeaker intercepts from linear mixed effects regression models have been used to index speaker advancement in sound change (Drager & Hay, 2012; Sóskuthy et al., 2017). The data we analyse here contains some non-linear effects, and thus an appropriate modelling technique to obtain the speaker intercepts from our data is generalised additive mixed modelling (GAMMs). This is because we want to model the normalised F1 and F2 of each of the 10 monophthongs separately, whilst also reliably capturing non-linear changes across time (see Winter & Wieling, 2016; Sóskuthy, 2017; Wieling, 2018 & Chuang et al., Chuang, Fon, & Baayen, 2020 for introductory tutorials on GAMMs). We fitted separate models to normalised F1 and F2 for each of the 10 vowels in the data set, giving 20 models in total.\nAll models were fitted using the same formula via the mgcv package in R (Wood, 2017), with fixed-effects comprising separate smooths over year of birth for each gender (using an adaptive smooth basis with 10 knots), as well as a smooth over speech rate. Speech rate is calculated as syllables per second for the transcript (where each speaker’s recording is spread across several transcripts. Individual transcripts contain on average approximately 6 min of speech). Random intercepts were included for speaker and word form. We then combined all of the speaker intercepts from the separate models, providing us with a final 481 X 20 data set, where each of the 481 speakers had a separate intercept for each of the 20 vocalic variables. (Brand et al. 2021, 8)\n\nHow did you report the results?\nIn the results section, the GAMMs are plotted, again in a vowel space: \nThe GAMMs are discussed as follows:\n\nWhile our primary purpose in fitting the GAMMs is to control for speaker factors such as year of birth, the major patterns revealed by the GAMMs also facilitate an understanding of sound change occurring over this time period, which is a necessary precursor to interpreting the patterns of co-variation. Fig. 5 shows the trajectories of vowel change, based on GAMM smooths over year of birth. All vowels have undergone some change, either linear or non-linear, based on the year of birth smooth effect (all p-values &lt; 0.001), see the Supplementary Materials for the model summaries. An animated and interactive version of the changes is available in the Shiny application. (Brand et al. 2021, 9)\n\nThe above notes statistical significance of the year of birth smooths and points readers to mode detail if required. The next paragraphs of the results section describe the patterns in the plot in light of existing literature.\nWhat is in supplementary material?\nThe supplementary material for the analysis is hosted on GitHub. It contains two relevant sections. First, a section with the code for fitting the models, which includes discussion of the model formula and the R code used to fit a series of models with the same formula (here). Second, there is a section containing all of the model summaries (here). These summaries do not contain information about random effects. We have seen above that this radically reduces the amount of time required to generate a model summary.\nIn addition, an online interactive is provided for exploring the analysis in the paper (including the GAMMs). See here.\n\n\nDerrick and Gick (2021) is a shorter piece for Scientific Reports. Space is at a premium so a short explanation is given.\nHow did you fit the GAMMs?\nThe discussion of the GAMMs in the methodology sections comes after an earlier set of steps have already been described. Justification is given in terms of non-linearity in the data.\n\nNext, and in order to make sense of this highly non-linear data, we ran a generalized additive mixed-effects model (GAMM) on the data shown in Eq. (7). GAMMs are extremely effective for the analysis of non-linear data, and are therefore highly suitable for the analysis of the critical fluctuations captured in Eq. (6).\nEquation (7), written in R-code, describes a generalized additive mixed-effects model, comparing Fluctuation based on tongue-front displacement (TFd) and the fluctuation time slice position (FTS), forming a 3-dimensional tensor (te) field [te(FTS, TFd)]. The random effects factor out participant variability in a 3-dimensional tensor field [s(FTS, TFd, Participant, bs = “fs”, m = 1)], as well as random-effect smooths for syllables per second [s(SPS, Participant, bs = “re”)], and token type [s(Token type, Participant, bs = “re”)]. In order to correct for autocorrelation effects, we ran the GAMM, calculated an estimate for a start value ρ from that first run, and provided that ρ to a second run of the GAMM model, along with an indicator identifying the first position of our time slices. This removes most of the autocorrelation, maximizing the accuracy of the resulting statistical model output.\nEquation (7) produces an output that shows the relationship between critical fluctuation, token position, and tongue-front displacement range, highlighting regions of significant difference. And with these methods, we were able to identify whether tongue-front displacement range affected speech-rate range, and whether tongue-front displacement range had any influence on the timing slice positions of critical fluctuations. (Derrick and Gick 2021, 8)\n\nEquation 7 is the following R formula:\n\ngam(Fluctuation ~ te(FTS, TFd) + \n      s(FTS, TFd, Participant, bs = \"fs\", m = 1) + \n      s(SPS, Participant, bs = \"re\") + \n      s(Tokentype, Participant, bs = \"re\")\n)\n\nNote, btw, the use of all both random smooths (the second smooth term) and two smooth terms defining random slopes (the third and fourth smooth terms in the model).\nThe methods sections describes the method used to deal with autocorrelation as well.\nFinally, a paragraph is given to explain the relationship between the output of the GAMMs and the actual subject matter of the research.\nCitations are given in footnotes to R itself, the mgcv package and the itsadug package.\nHow did you report the results?\nThere results section reports both the significance of smooth terms and discusses a plot.\nThe main plot is Figure 7:\n The discussion is as follows:\n\nGeneralized additive mixed-effects model analysis of critical fluctuations during the time course of token production by tongue-front displacement range are shown in Fig. 7. The model shows that speakers producing tokens with the lowest tongue-front displacement ranges have relatively higher critical fluctuations in the early part of their token productions, spanning from the first vowel through the first flap into the middle of the second vowel. In contrast, they show much lower rates of critical fluctuation from the second half of the second vowel, through the second flap to the end of the third vowel. This constitutes evidence of end-state comfort effects for speakers producing token types with narrow tongue-front displacement ranges.\nFor speakers producing token types in the middle of the group, there are no end-state comfort effects, but instead the most effort made during the second flap. For speakers with very wide tongue-front displacement ranges, there is again statistically significant evidence for end-state comfort effects, with extra beginning-state effort for the initial vowel. These are the same speakers producing token types that demonstrate two categorically different patterns of motion—one for slow speech, and one for fast speech.\nFigure 7 shows the regions of significance for the GAMM whose model output is shown in Table 3. These results show that all of the model parameters are significant, and most importantly that the tensor field shown in Fig. 7 accounts for a significant portion of the variance of the data. This includes the fixed-effect tensor relating tongue-front displacement range and critical fluctuation along time slices, as well as the random-effects for participant, token type, and reiterant speech rate. The entire GAMM accounts for an adjusted r2 of 0.452, explaining 47% of the deviance in critical fluctuations in this dataset. (Derrick and Gick 2021, 10–11)\n\nSignificance information is given as Table 3.\n\n\n\nTable 3 from Derrick and Gick (2021).\n\n\nWhat is in supplementary materials?\nThere is a repository on OSF (here). Code is available as an Rmarkdown or as an HTML file to be downloaded and read in the browser.\nThe analysis shows that the p-values in Table 3 are generated by using itsadug::compareML() and fitting the models with maximum likelihood estimation. With more space, this might have been discussed in the paper. But there is not always more space!\n\n\n\nWhat can we take from the above examples? Here are some reflections:\n\nYou don’t need to say everything in the paper. You can use supplementary material. This should include, at a minimum, the specific code you used to fit the model and generate any reported p-values (not just the summary).\n\nThis can be important for seeing, e.g., whether the ‘difference smooth’ structure was used (see discussion above).\n\nCite the specific packages you used. The function citation() in R can be very helpful for this (e.g. look at the results of citation('mgcv')). I often use the package grateful to generate a bibliography of all of the packages I use within an analysis project.\nVisualisations are an important part of evaluating GAMMs (moreso than for linear models where a single coefficient is what you are testing).\n\nYour discussion section might consist of description of the specific trajectories you find visually.\n\nSupplementary material opens up many windows (including e.g., the interactive application in Brand et al. (2021)). This is also, however, a potential time sync. Try to balance the costs and benefits when you go beyond the requirements of open science and into the realm of web development!",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#sec-resources",
    "href": "gamms_1.html#sec-resources",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "5.7 Further Resources",
    "text": "5.7 Further Resources\n\nMárton Sóskuthy’s GAMM tutorial: https://arxiv.org/pdf/1703.05339.pdf\nMárton Sóskuthy’s paper compared multiple significance testing strategies for error rates: https://www.sciencedirect.com/science/article/pii/S009544702030108X#s0070\nMartijn Wieling’s tutorial: https://www.sciencedirect.com/science/article/pii/S0095447017301377\n\n\n\n\n\n\n\nNote\n\n\n\nResources have also been written by non-Martins. I will add some soon!\n\n\n\n\n\n\nBrand, James, Jen Hay, Lynn Clark, Kevin Watson, and Márton Sóskuthy. 2021. “Systematic Co-Variation of Monophthongs Across Speakers of New Zealand English.” Journal of Phonetics 88: 101096. https://www.sciencedirect.com/science/article/pii/S0095447021000711.\n\n\nDerrick, Donald, and Bryan Gick. 2021. “Gait Change in Tongue Movement.” Scientific Reports 11 (1): 16565. https://doi.org/10.1038/s41598-021-96139-4.\n\n\nRenwick, Margaret E. L., Joseph A. Stanley, Jon Forrest, and Lelia Glass. 2023. “Boomer Peak or Gen X Cliff? From SVS to LBMS in Georgia English.” Language Variation and Change 35 (2): 175–97. https://doi.org/10.1017/S095439452300011X.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction.” arXiv.org. March 15, 2017. https://arxiv.org/abs/1703.05339v1.\n\n\nSóskuthy, Márton, J. Hay, and James Brand. 2019. “Horizontal Diphthong Shift in New Zealand English.” In. https://www.semanticscholar.org/paper/HORIZONTAL-DIPHTHONG-SHIFT-IN-NEW-ZEALAND-ENGLISH-S%C3%B3skuthy-Hay/cd1bd700686b3d1270be5536e5881e8946ba57ab.\n\n\nWieling, M. 2018. “Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: A Tutorial Focusing on Articulatory Differences Between L1 and L2 Speakers of English.” Journal of Phonetics 70: 86–116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWilson Black, Joshua, Jennifer Hay, Lynn Clark, and James Brand. 2023. “The Overlooked Effect of Amplitude on Within-Speaker Vowel Variation.” Linguistics Vanguard 9 (1): 173–89. https://doi.org/10.1515/lingvan-2022-0086.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "gamms_1.html#footnotes",
    "href": "gamms_1.html#footnotes",
    "title": "5  Generalized Additive (Mixed) Models",
    "section": "",
    "text": "If you are not familiar with this bit of linguistics, just think of this a set of readings derived from a single action. It could just as easily be a set of readings from a brain scan, or a moving slider, or whatever you like. The only restriction is that we have a set of measurements across time of the same event or action.↩︎\nWe will look at how to do this later in the series.↩︎",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brand, James, Jen Hay, Lynn Clark, Kevin Watson, and Márton Sóskuthy.\n2021. “Systematic Co-Variation of Monophthongs Across Speakers of\nNew Zealand English.” Journal of Phonetics\n88: 101096. https://www.sciencedirect.com/science/article/pii/S0095447021000711.\n\n\nDerrick, Donald, and Bryan Gick. 2021. “Gait Change in Tongue\nMovement.” Scientific Reports 11 (1): 16565. https://doi.org/10.1038/s41598-021-96139-4.\n\n\nRenwick, Margaret E. L., Joseph A. Stanley, Jon Forrest, and Lelia\nGlass. 2023. “Boomer Peak or Gen X\nCliff? From SVS to LBMS in\nGeorgia English.” Language Variation and\nChange 35 (2): 175–97. https://doi.org/10.1017/S095439452300011X.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for\nDynamic Analysis in Linguistics: A Practical Introduction.”\narXiv.org. March 15, 2017. https://arxiv.org/abs/1703.05339v1.\n\n\nSóskuthy, Márton, J. Hay, and James Brand. 2019. “Horizontal\nDiphthong Shift in New Zealand English.” In. https://www.semanticscholar.org/paper/HORIZONTAL-DIPHTHONG-SHIFT-IN-NEW-ZEALAND-ENGLISH-S%C3%B3skuthy-Hay/cd1bd700686b3d1270be5536e5881e8946ba57ab.\n\n\nWieling, M. 2018. “Analyzing Dynamic Phonetic Data Using\nGeneralized Additive Mixed Modeling: A Tutorial Focusing on\nArticulatory Differences Between L1 and L2\nSpeakers of English.” Journal of Phonetics\n70: 86–116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWilson Black, Joshua, Jennifer Hay, Lynn Clark, and James Brand. 2023.\n“The Overlooked Effect of Amplitude on Within-Speaker Vowel\nVariation.” Linguistics Vanguard 9 (1): 173–89. https://doi.org/10.1515/lingvan-2022-0086.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An\nIntroduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "References"
    ]
  }
]